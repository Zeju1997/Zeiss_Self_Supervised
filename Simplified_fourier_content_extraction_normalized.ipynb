{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d652ea",
   "metadata": {},
   "source": [
    "# Using FDA to evaluate image style transfer\n",
    "\n",
    "This notebook is based upon `FDA: Fourier Domain Adaptation for Semantic Segmentation` from Yanchao Yang.\n",
    "\n",
    "Before starting, the original data and the transferred images should be stored in a known path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36cbed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def high_freq_mutate( amp_src, amp_trg, L=0.1 ):\n",
    "    a_src = torch.fft.fftshift( amp_src, dim =(-2, -1) )\n",
    "    a_trg = torch.fft.fftshift( amp_trg, dim =(-2, -1) )\n",
    "\n",
    "    c, h, w = a_src.shape\n",
    "    b = (  np.floor(np.amin((h,w))*L)  ).astype(int)\n",
    "    c_h = np.floor(h/2.0).astype(int)\n",
    "    c_w = np.floor(w/2.0).astype(int)\n",
    "\n",
    "    h1 = c_h-b\n",
    "    h2 = c_h+b+1\n",
    "    w1 = c_w-b\n",
    "    w2 = c_w+b+1\n",
    "\n",
    "    a_src[:,h1:h2,w1:w2] = a_trg[:,h1:h2,w1:w2]\n",
    "    a_src = torch.fft.ifftshift( a_src, dim =(-2, -1) )\n",
    "    return a_src\n",
    "\n",
    "def FDA_source_to_target(src_img, trg_img, L=0.1):\n",
    "    # exchange magnitude\n",
    "    # input: src_img, trg_img\n",
    "\n",
    "    src_img_torch = src_img.clone() #.cpu().numpy()\n",
    "    trg_img_torch = trg_img.clone() #.cpu().numpy()\n",
    "\n",
    "    # get fft of both source and target\n",
    "    fft_src_torch = torch.fft.rfft2( src_img_torch, dim=(-2, -1) )\n",
    "    fft_trg_torch = torch.fft.rfft2( trg_img_torch, dim=(-2, -1) )\n",
    "\n",
    "    # extract amplitude and phase of both ffts\n",
    "    amp_src, pha_src = torch.abs(fft_src_torch), torch.angle(fft_src_torch)\n",
    "    amp_trg, pha_trg = torch.abs(fft_trg_torch), torch.angle(fft_trg_torch)\n",
    "\n",
    "    # mutate the amplitude part of source with target\n",
    "    amp_src_ = high_freq_mutate( amp_src, amp_trg, L=L )\n",
    "\n",
    "    # mutated fft of source\n",
    "    fft_src_ = amp_src_ * torch.exp( 1j * pha_src )\n",
    "\n",
    "    # get the mutated image\n",
    "    src_in_trg = torch.fft.irfft2( fft_src_, dim=(-2, -1) )\n",
    "    #src_in_trg = torch.real(src_in_trg)\n",
    "\n",
    "    return src_in_trg\n",
    "\n",
    "def FDA_distance_torch( src_img, src2trg_img, L=0.1 , normalize = False, display = False):\n",
    "    # exchange magnitude\n",
    "    # input: src_img, trg_img\n",
    "\n",
    "    src_img_torch = src_img.clone() #.cpu().numpy()\n",
    "    src2trg_img_torch = src2trg_img.clone() #.cpu().numpy()\n",
    "\n",
    "    # get fft of both source and target\n",
    "    fft_src_torch = torch.fft.rfft2( src_img_torch, dim=(-2, -1) )\n",
    "    fft_trg_torch = torch.fft.rfft2( src2trg_img_torch, dim=(-2, -1) )\n",
    "\n",
    "    # extract amplitude and phase of both ffts\n",
    "    amp_src, pha_src = torch.abs(fft_src_torch), torch.angle(fft_src_torch)\n",
    "    amp_trg, pha_trg = torch.abs(fft_trg_torch), torch.angle(fft_trg_torch)\n",
    "\n",
    "    # mutate the amplitude part of source with target\n",
    "    low_freq_part, a_src, a_trg = high_freq_part_torch( amp_src, amp_trg, L=L, normalize = normalize )\n",
    "\n",
    "    low_freq_dist_fro = torch.linalg.norm(torch.flatten(low_freq_part))\n",
    "\n",
    "    low_freq_dist_L1 = torch.linalg.norm(torch.flatten(low_freq_part), ord = 1)\n",
    "\n",
    "    low_freq_dist_inf = torch.linalg.norm(torch.flatten(low_freq_part), ord = float('inf'))\n",
    "    \n",
    "    low_freq_dist = (low_freq_dist_fro, low_freq_dist_L1, low_freq_dist_inf)\n",
    "    \n",
    "    if display:\n",
    "\n",
    "        # mutated fft of source\n",
    "        fft_src_ = a_src * torch.exp( 1j * pha_src )\n",
    "        # mutated fft of source\n",
    "        fft_trg_ = a_trg * torch.exp( 1j * pha_trg )\n",
    "        \n",
    "        low_freq_part_src_ = low_freq_part * torch.exp( 1j * pha_src )\n",
    "        low_freq_part_trg_ = low_freq_part * torch.exp( 1j * pha_trg )\n",
    "        \n",
    "        src_wo_style = torch.fft.irfft2( fft_src_, dim=(-2, -1) )\n",
    "        trg_wo_style = torch.fft.irfft2( fft_trg_, dim=(-2, -1) )\n",
    "        low_freq_part_ifft = torch.fft.irfft2( low_freq_part, dim=(-2, -1) )\n",
    "        low_freq_part_src_ = torch.fft.irfft2( low_freq_part_src_, dim=(-2, -1) )\n",
    "        low_freq_part_trg_ = torch.fft.irfft2( low_freq_part_trg_, dim=(-2, -1) )\n",
    "        \n",
    "        low_freq_tuple = (low_freq_part, low_freq_part_ifft, low_freq_part_src_, low_freq_part_trg_)\n",
    "        \n",
    "        return low_freq_dist, low_freq_tuple, src_wo_style, trg_wo_style\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        return low_freq_dist\n",
    "\n",
    "def high_freq_part_torch( amp_src, amp_trg, L=0.1, normalize = False):\n",
    "    a_src = torch.fft.fftshift( amp_src, dim =(-2, -1) )\n",
    "    a_trg = torch.fft.fftshift( amp_trg, dim =(-2, -1) )\n",
    "    \n",
    "    \n",
    "    #print(a_src.shape)\n",
    "    #print(a_trg.shape)\n",
    "    \n",
    "    max_src_temp = a_src.max(dim = 1)\n",
    "    max_trg_temp = a_trg.max(dim = 1)\n",
    "    max_src = max_src_temp.values.max(dim = 1)\n",
    "    max_trg = max_trg_temp.values.max(dim = 1)\n",
    "    #print(max_src.values)\n",
    "    #print(max_trg.values)\n",
    "    #print(a_src.max())\n",
    "    #print(a_trg.max())\n",
    "\n",
    "    c, h, w = a_src.shape\n",
    "    b = (  np.floor(np.amin((h,w))*L)  ).astype(int)\n",
    "    c_h = np.floor(h/2.0).astype(int)\n",
    "    c_w = np.floor(w/2.0).astype(int)\n",
    "\n",
    "    h1 = c_h-b\n",
    "    h2 = c_h+b+1\n",
    "    w1 = c_w-b\n",
    "    w2 = c_w+b+1\n",
    "    \n",
    "    a_src[:,h1:h2,w1:w2] = a_src[:,h1:h2,w1:w2] * 0\n",
    "    a_trg[:,h1:h2,w1:w2] = a_trg[:,h1:h2,w1:w2] * 0\n",
    "    \n",
    "    if normalize:\n",
    "    \n",
    "        if 0 not in max_src.values:\n",
    "            low_freq_part = torch.div(a_src.permute((1, 2, 0)), max_src.values) - torch.div(a_trg.permute((1, 2, 0)), max_trg.values)\n",
    "        else:\n",
    "            low_freq_part = a_src * 0\n",
    "            \n",
    "        #print(low_freq_part.shape)\n",
    "            \n",
    "        low_freq_part = low_freq_part / ((2 * b) * (2 * b))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        low_freq_part = a_src - a_trg\n",
    "        \n",
    "    \n",
    "    a_src = torch.fft.ifftshift( a_src, dim =(-2, -1) )\n",
    "    a_trg = torch.fft.ifftshift( a_trg, dim =(-2, -1) )\n",
    "    \n",
    "    low_freq_part = torch.fft.ifftshift( low_freq_part, dim =(-2, -1) )\n",
    "    \n",
    "    return low_freq_part, a_src, a_trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e13c786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gta5 dataset (source)\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "'''\n",
    "labels = [\n",
    "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
    "    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
    "    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
    "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
    "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
    "    Label(  'parking'              ,  9 ,      255 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
    "    Label(  'rail track'           , 10 ,      255 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
    "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
    "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
    "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
    "    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
    "    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
    "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
    "    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
    "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
    "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
    "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
    "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
    "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
    "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
    "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
    "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
    "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
    "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
    "    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
    "    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
    "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
    "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
    "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
    "    Label(  'license plate'        , -1 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
    "]\n",
    "'''\n",
    "\n",
    "class GTA5Dataset(data.Dataset):\n",
    "    def __init__(self, root, list_path, max_iters=None, crop_size=(256, 256), mean=(128, 128, 128), ignore_label=255):\n",
    "        self.root = root\n",
    "        self.list_path = list_path\n",
    "        self.crop_size = crop_size\n",
    "        self.ignore_label = ignore_label\n",
    "        self.mean = mean\n",
    "        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n",
    "        if not max_iters==None:\n",
    "            self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))\n",
    "        self.files = []\n",
    "\n",
    "        self.id_to_trainid = {7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,\n",
    "                              19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12,\n",
    "                              26: 13, 27: 14, 28: 15, 31: 16, 32: 17, 33: 18}\n",
    "\n",
    "        self.id2label = {-1: ignore_label, 0: ignore_label, 1: ignore_label, 2: ignore_label,\n",
    "            3: ignore_label, 4: ignore_label, 5: ignore_label, 6: ignore_label,\n",
    "            7: 0, 8: 1, 9: ignore_label, 10: ignore_label, 11: 2, 12: 3, 13: 4,\n",
    "            14: ignore_label, 15: ignore_label, 16: ignore_label, 17: 5,\n",
    "            18: ignore_label, 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14,\n",
    "            28: 15, 29: ignore_label, 30: ignore_label, 31: 16, 32: 17, 33: 18}\n",
    "\n",
    "        #self.id_to_trainid = {7: 1, 24: 2, 26: 3} #Road/car/people\n",
    "        self.id_to_trainid = {11: 1, 24: 2, 21: 3} #Building/car/vegetation\n",
    "        #self.ignore_label = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name = self.img_ids[index]\n",
    "        \n",
    "        image = Image.open(osp.join(self.root, \"images/%s\" % name)).convert('RGB')\n",
    "        label = Image.open(osp.join(self.root, \"labels/%s\" % name))\n",
    "        # resize\n",
    "        image = image.resize(self.crop_size, Image.BICUBIC)\n",
    "        label = label.resize(self.crop_size, Image.NEAREST)\n",
    "\n",
    "        image = np.asarray(image, np.float32)\n",
    "        label = np.asarray(label, np.int8)\n",
    "\n",
    "        label_copy = self.ignore_label * np.ones(label.shape, dtype=np.float32)\n",
    "        #for k, v in self.id_to_trainid.items():\n",
    "        for k, v in self.id2label.items():\n",
    "            label_copy[label == k] = v\n",
    "        size = image.shape\n",
    "        #image = image[:, :, ::-1]  # change to BGR\n",
    "        #image -= self.mean\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        sample = {'image': image.copy(),\n",
    "                  'label': label_copy.copy()}\n",
    "\n",
    "        return sample\n",
    "    \n",
    "class GTA5Dataset1(data.Dataset):\n",
    "    def __init__(self, root, list_path, max_iters=None, crop_size=(256, 256), mean=(128, 128, 128), ignore_label=255):\n",
    "        self.root = root\n",
    "        self.list_path = list_path\n",
    "        self.crop_size = crop_size\n",
    "        self.ignore_label = ignore_label\n",
    "        self.mean = mean\n",
    "        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n",
    "        if not max_iters==None:\n",
    "            self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))\n",
    "        self.files = []\n",
    "\n",
    "        self.id_to_trainid = {7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,\n",
    "                              19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12,\n",
    "                              26: 13, 27: 14, 28: 15, 31: 16, 32: 17, 33: 18}\n",
    "\n",
    "        self.id2label = {-1: ignore_label, 0: ignore_label, 1: ignore_label, 2: ignore_label,\n",
    "            3: ignore_label, 4: ignore_label, 5: ignore_label, 6: ignore_label,\n",
    "            7: 0, 8: 1, 9: ignore_label, 10: ignore_label, 11: 2, 12: 3, 13: 4,\n",
    "            14: ignore_label, 15: ignore_label, 16: ignore_label, 17: 5,\n",
    "            18: ignore_label, 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14,\n",
    "            28: 15, 29: ignore_label, 30: ignore_label, 31: 16, 32: 17, 33: 18}\n",
    "\n",
    "        #self.id_to_trainid = {7: 1, 24: 2, 26: 3} #Road/car/people\n",
    "        self.id_to_trainid = {11: 1, 24: 2, 21: 3} #Building/car/vegetation\n",
    "        #self.ignore_label = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name = self.img_ids[index]\n",
    "                \n",
    "        name = name[:-4] + \".jpg\"\n",
    "        \n",
    "        image = Image.open(osp.join(self.root, \"images/%s\" % name)).convert('RGB')\n",
    "        \n",
    "        name = name[:-4] + \".png\"\n",
    "        label = Image.open(osp.join(self.root, \"labels/%s\" % name))\n",
    "        # resize\n",
    "        image = image.resize(self.crop_size, Image.BICUBIC)\n",
    "        label = label.resize(self.crop_size, Image.NEAREST)\n",
    "\n",
    "        image = np.asarray(image, np.float32)\n",
    "        label = np.asarray(label, np.int8)\n",
    "\n",
    "        label_copy = self.ignore_label * np.ones(label.shape, dtype=np.float32)\n",
    "        #for k, v in self.id_to_trainid.items():\n",
    "        for k, v in self.id2label.items():\n",
    "            label_copy[label == k] = v\n",
    "        size = image.shape\n",
    "        #image = image[:, :, ::-1]  # change to BGR\n",
    "        #image -= self.mean\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        sample = {'image': image.copy(),\n",
    "                  'label': label_copy.copy()}\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce58a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cityscapes dataset (target)\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "palette = [128, 64, 128, 244, 35, 232, 70, 70, 70, 102, 102, 156, 190, 153, 153, 153, 153, 153, 250, 170, 30,\n",
    "            220, 220, 0, 107, 142, 35, 152, 251, 152, 70, 130, 180, 220, 20, 60, 255, 0, 0, 0, 0, 142, 0, 0, 70,\n",
    "            0, 60, 100, 0, 80, 100, 0, 0, 230, 119, 11, 32]\n",
    "classes = ['road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light', 'traffic sign',\n",
    "        'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck', 'bus', 'train', 'motorcycle',\n",
    "        'bicycle']\n",
    "\n",
    "    \n",
    "class CityscapesDataset(data.Dataset):\n",
    "    def __init__(self, root, list_path, max_iters=None, crop_size=(256, 256), mean=(128, 128, 128), ignore_label=255):\n",
    "        self.root = root\n",
    "        self.list_path = list_path\n",
    "        self.crop_size = crop_size\n",
    "        self.ignore_label = ignore_label\n",
    "        self.mean = mean\n",
    "        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n",
    "        if not max_iters==None:\n",
    "            self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))\n",
    "        self.files = []\n",
    "        \n",
    "        self.id_to_trainid = {7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,\n",
    "                              19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12,\n",
    "                              26: 13, 27: 14, 28: 15, 31: 16, 32: 17, 33: 18}\n",
    "\n",
    "        self.id2label = {-1: ignore_label, 0: ignore_label, 1: ignore_label, 2: ignore_label,\n",
    "            3: ignore_label, 4: ignore_label, 5: ignore_label, 6: ignore_label,\n",
    "            7: 0, 8: 1, 9: ignore_label, 10: ignore_label, 11: 2, 12: 3, 13: 4,\n",
    "            14: ignore_label, 15: ignore_label, 16: ignore_label, 17: 5,\n",
    "            18: ignore_label, 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14,\n",
    "            28: 15, 29: ignore_label, 30: ignore_label, 31: 16, 32: 17, 33: 18}\n",
    "\n",
    "        #self.id_to_trainid = {7: 1, 24: 2, 26: 3} #Road/car/people\n",
    "        self.id_to_trainid = {11: 1, 24: 2, 21: 3} #Building/car/vegetation\n",
    "        #self.ignore_label = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name = self.img_ids[index]\n",
    "        image_root = osp.join(self.root, 'cityscapes')\n",
    "        label_root = osp.join(self.root, 'gtFine')\n",
    "        image = Image.open(osp.join(image_root, \"%s\" % name)).convert('RGB')\n",
    "        label = Image.open(osp.join(label_root, \"%s\" % name.replace(\"leftImg8bit\", \"gtFine_labelIds\")))\n",
    "        # resize\n",
    "        image = image.resize(self.crop_size, Image.BICUBIC)\n",
    "        label = label.resize(self.crop_size, Image.NEAREST)\n",
    "\n",
    "        image = np.asarray(image, np.float32)\n",
    "        label = np.asarray(label, np.int8)\n",
    "\n",
    "        label_copy = self.ignore_label * np.ones(label.shape, dtype=np.float32)\n",
    "        #for k, v in self.id_to_trainid.items():\n",
    "        for k, v in self.id2label.items():\n",
    "            label_copy[label == k] = v\n",
    "        size = image.shape\n",
    "        #image = image[:, :, ::-1]  # change to BGR\n",
    "        #image -= self.mean\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        sample = {'image': image.copy(),\n",
    "                  'label': label_copy.copy()}\n",
    "\n",
    "        return sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "577fb1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31457/1949966721.py:6: DeprecationWarning: Please use `zoom` from the `scipy.ndimage` namespace, the `scipy.ndimage.interpolation` namespace is deprecated.\n",
      "  from scipy.ndimage.interpolation import zoom\n"
     ]
    }
   ],
   "source": [
    "import skimage.io as io\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from torch.utils.data import Dataset\n",
    "# from medpy.io import load\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image  # using pillow-simd for increased speed\n",
    "\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning\n",
    "    # (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert(\"L\")\n",
    "\n",
    "class Retouch_dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 base_dir,\n",
    "                 list_dir,\n",
    "                 size=(512, 512),\n",
    "                 split='train',\n",
    "                 is_train=False,\n",
    "                 transform=None,\n",
    "                 ext='.png'):\n",
    "        self.transform = transform  # using transform in torch!\n",
    "        self.split = split\n",
    "        if split == '':\n",
    "            self.sample_list = open(list_dir).readlines()\n",
    "        else:\n",
    "            self.sample_list = open(os.path.join(list_dir +\n",
    "                                                 self.split + '.txt')).readlines()\n",
    "        self.data_dir = base_dir\n",
    "        self.loader = pil_loader\n",
    "        self.to_tensor = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size),\n",
    "                transforms.ToTensor(),\n",
    "                #transforms.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "            ])\n",
    "\n",
    "        self.is_train = is_train\n",
    "        self.transform = transform\n",
    "        self.ext = ext\n",
    "\n",
    "    def augment(self, data, label):\n",
    "        data_label = torch.cat((data, label), dim=0)\n",
    "        data_label_aug = self.transform(data_label)\n",
    "        data_aug = data_label_aug[0, :, :].unsqueeze(0)\n",
    "        label_aug = data_label_aug[1, :, :].unsqueeze(0)\n",
    "        return data_aug, label_aug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_name = self.sample_list[idx].strip('\\n')\n",
    "\n",
    "        vendor = sample_name.split(' ')[0]\n",
    "        slice_name = sample_name.split(' ')[1]\n",
    "        slice_idx = sample_name.split(' ')[2].zfill(3)\n",
    "\n",
    "        data_path = os.path.join(self.data_dir,\n",
    "                                 vendor,\n",
    "                                 slice_name,\n",
    "                                 'images',\n",
    "                                 slice_idx + self.ext)\n",
    "        '''\n",
    "\n",
    "        label_path = os.path.join(self.data_dir,\n",
    "                                  vendor,\n",
    "                                  slice_name,\n",
    "                                  'labels',\n",
    "                                  slice_idx + '.npy')\n",
    "\n",
    "        label = torch.from_numpy(np.load(label_path))\n",
    "        label_idx = torch.argmax(label, dim=0, keepdim=True)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        data = self.to_tensor(self.loader(data_path))\n",
    "        transform_avaliable = self.transform is not None and self.is_train\n",
    "        do_aug = transform_avaliable and random.random() > 0.5\n",
    "\n",
    "        if do_aug:\n",
    "            # data, label_idx = self.augment(data, label_idx)\n",
    "            data = self.augment(data)\n",
    "\n",
    "        # label_idx = label_idx.squeeze(0).long()\n",
    "\n",
    "        sample = {'image': data,\n",
    "                  'case_name': sample_name}\n",
    "        # print((label_idx==0).sum()/512**2)\n",
    "        return sample\n",
    "\n",
    "# Test Unit\n",
    "# flip = transforms.RandomHorizontalFlip(p=0.5)\n",
    "# base_dir = 'Retouch-dataset_test/pre_processed/'\n",
    "# list_dir = ''\n",
    "# dataset = Retouch_dataset(base_dir, list_dir, transform=flip)\n",
    "# l = dataset[3]['label']\n",
    "# d = dataset[3]['image']\n",
    "#\n",
    "# print(l.shape, d.shape)\n",
    "#\n",
    "# img = d.permute(1, 2, 0).numpy()\n",
    "# print((img[:, :, 0] == img[:, :, 2]).all())\n",
    "# print(dataset[3]['case_name'])\n",
    "# plt.figure()\n",
    "# plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33d75967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset_mode, folder_name, split_name, split, size):\n",
    "    if dataset_mode == \"retouch\":\n",
    "        source_dataset = Retouch_dataset(base_dir=folder_name, list_dir=split_name, split='', size = crop_size)\n",
    "    elif dataset_mode == \"gta5\":\n",
    "        source_dataset = GTA5Dataset(root=folder_name, list_path=split_name, crop_size=size, ignore_label=19)\n",
    "    elif dataset_mode == \"cityscapes\":\n",
    "        source_dataset = CityscapesDataset(root=folder_name, list_path=split_name, crop_size=size, ignore_label=19)\n",
    "    else:\n",
    "        print(\"Unrecognized dataset!\")\n",
    "        sys.exit()\n",
    "        \n",
    "    return source_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8063a7",
   "metadata": {},
   "source": [
    "## Using the custom data in VISSL\n",
    "\n",
    "The original data is saved in the `data` directory. The transferred images are saved in such a way, that they are stored in the `data/transferred/#epoch` directory (`#epoch` is the number of CycleGAN epoch).\n",
    "\n",
    "**EXAMPLE 1**: download the retouch data set from [retouch-dataset](https://drive.google.com/file/d/1r8pQCoVzEAHdy9wLW_MUkyfgBBFePMPv/view?usp=sharing) and insert it into the `data` directory. Download the transferred images from [transferred-retouch-images](https://drive.google.com/file/d/1nMcyF-z2yvPBDY70qBsT2Ydg7NUITpmR/view?usp=sharing) and isert the subfolders with the epoch number into the `data/transferred` directory.\n",
    "\n",
    "**EXAMPLE 2**: download the truncated retouch GTAV data set from [gta5-truncated-dataset](https://drive.google.com/file/d/1R9zmrwAKf03KOq9MSfhdPd6xOVRGEtrY/view?usp=sharing) and insert it into the `data` directory. Download the transferred images from [transferred-gta5-images](https://drive.google.com/file/d/1SLdGNHDi3LZTHXXNMNFDTmAQibAEjj-x/view?usp=sharing) and isert the subfolders with the epoch number into the `data/transferred` directory. Note, it also works with the whole data set, one only has to change the `splits/gta5/gta5.txt` to the whole dataset. The truncated version is used due to memory and time efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6d50deb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0 :\n",
      "Loading from: 'data/fourier/0/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2497/2497 [00:54<00:00, 45.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean content FT : 0.005243747097311947 (mean fro), 0.6335962659294815 (mean L1), 0.0009937944220080135 (mean inf)\n",
      "Var content FT : 2.9616228238586528e-05 (var fro), 0.21553011664875726 (var L1), 1.621874765788993e-06 (var inf)\n",
      "Starting epoch 2 :\n",
      "Loading from: 'data/fourier/2/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2497/2497 [00:52<00:00, 47.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean content FT : 0.08665429896380025 (mean fro), 8.078563885255294 (mean L1), 0.016400589254074182 (mean inf)\n",
      "Var content FT : 0.0005044099326271049 (var fro), 1.8853676441818012 (var L1), 4.1156961754610546e-05 (var inf)\n",
      "Starting epoch 4 :\n",
      "Loading from: 'data/fourier/4/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2497/2497 [00:52<00:00, 47.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean content FT : 0.06765801955758928 (mean fro), 6.309012919556966 (mean L1), 0.013660774840277716 (mean inf)\n",
      "Var content FT : 0.0004790678863079971 (var fro), 1.5331150619064098 (var L1), 4.1240441315447285e-05 (var inf)\n",
      "Starting epoch 6 :\n",
      "Loading from: 'data/fourier/6/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2497/2497 [00:52<00:00, 47.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean content FT : 0.06665206196821233 (mean fro), 6.242947189150021 (mean L1), 0.012878599438743952 (mean inf)\n",
      "Var content FT : 0.0003017592231936984 (var fro), 1.0474466145288612 (var L1), 2.5153750087352576e-05 (var inf)\n",
      "Starting epoch 8 :\n",
      "Loading from: 'data/fourier/8/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2497/2497 [00:53<00:00, 47.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean content FT : 0.06708000184245286 (mean fro), 6.003102911440048 (mean L1), 0.014205187024745582 (mean inf)\n",
      "Var content FT : 0.0003052561902349813 (var fro), 0.8679299992215921 (var L1), 3.182760228679323e-05 (var inf)\n",
      "Starting epoch 10 :\n",
      "Loading from: 'data/fourier/10/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2497/2497 [00:54<00:00, 45.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean content FT : 0.07998240082369931 (mean fro), 6.301456551003752 (mean L1), 0.017078530196470466 (mean inf)\n",
      "Var content FT : 0.0004496712571328838 (var fro), 0.8839361258553314 (var L1), 3.984291919705258e-05 (var inf)\n",
      "Starting epoch 12 :\n",
      "Loading from: 'data/fourier/12/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2497/2497 [00:56<00:00, 44.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean content FT : 0.07259896933856395 (mean fro), 6.943597739249838 (mean L1), 0.014590108144467396 (mean inf)\n",
      "Var content FT : 0.00034857091743702764 (var fro), 1.368007667142607 (var L1), 3.355426685897053e-05 (var inf)\n",
      "Starting epoch 14 :\n",
      "Loading from: 'data/fourier/14/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2497/2497 [00:54<00:00, 45.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean content FT : 0.082871966961358 (mean fro), 6.566706064800573 (mean L1), 0.017814391563599128 (mean inf)\n",
      "Var content FT : 0.0004836313141310321 (var fro), 1.1570412048222216 (var L1), 4.486242933471048e-05 (var inf)\n",
      "Starting epoch 16 :\n",
      "Loading from: 'data/fourier/16/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2497/2497 [00:52<00:00, 47.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean content FT : 0.07060923156786142 (mean fro), 6.372371600587988 (mean L1), 0.014368668416030869 (mean inf)\n",
      "Var content FT : 0.0004509931211690666 (var fro), 1.375853377443662 (var L1), 4.106327613230687e-05 (var inf)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# OCT torch implementation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import data\n",
    "\n",
    "dataset_mode =\"retouch\" # retouch / gta5\n",
    "\n",
    "transferred_images_dir = os.path.join(os.getcwd(), \"data/transferred\")\n",
    "epochs = [int(f) for f in os.listdir(transferred_images_dir) if os.path.isdir(os.path.join(transferred_images_dir, f))]\n",
    "epochs.sort()\n",
    "\n",
    "head = \"results/{}/\".format(dataset_mode)\n",
    "\n",
    "if not os.path.exists(head):\n",
    "    os.makedirs(head)\n",
    "    \n",
    "folder_name = \"data/{}\".format(dataset_mode)\n",
    "split_name = \"splits/{}/{}.txt\".format(dataset_mode, dataset_mode)\n",
    "split = \"\"\n",
    "crop_size = (256, 256)\n",
    "batch_size = 1\n",
    "batch = 50000\n",
    "\n",
    "source_dataset = create_dataset(dataset_mode, folder_name, split_name, split, crop_size)\n",
    "source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#split_name = \"./splits/gta5/gta5_list.txt\"\n",
    "#crop_size = (512, 512)\n",
    "#batch_size = 1\n",
    "\n",
    "device = torch.device('cuda:{}'.format(0))\n",
    "\n",
    "for L in [0.01]:\n",
    "#for L in [0.05]:\n",
    "\n",
    "    to_write = []\n",
    "    title = [\"epoch\", \"mean fro\", \"var fro\", \"mean L1\", \"var L1\", \"mean inf\", \"var inf\"]\n",
    "    to_write.append(title)\n",
    "\n",
    "    for epoch in epochs:\n",
    "        print(\"Starting epoch {} :\".format(epoch))\n",
    "        folder_name = \"data/fourier/{}/\".format(epoch)  # Replace with your produced GTA5 dir !\n",
    "        print(\"Loading from: '\" + folder_name + \"'\")\n",
    "        source2target_dataset = create_dataset(dataset_mode, folder_name, split_name, split, crop_size)\n",
    "        source2target_loader = DataLoader(source2target_dataset, batch_size=batch_size, shuffle=False)\n",
    "        source2target_loader_iter = iter(source2target_loader)\n",
    "        FDA_distances_fro = []\n",
    "        FDA_distances_L1 = []\n",
    "        FDA_distances_inf = []\n",
    "\n",
    "        for i, data in enumerate(tqdm(source_loader)):\n",
    "            if i < batch:\n",
    "                source = data[\"image\"].cuda().to(device)\n",
    "                source_img = source[0]\n",
    "\n",
    "                source2target = source2target_loader_iter.next()[\"image\"].cuda().to(device)\n",
    "                source2target_img = source2target[0]\n",
    "\n",
    "                FDA_distance = FDA_distance_torch(src_img = source_img, \n",
    "                                                  src2trg_img = source2target_img, \n",
    "                                                  L = L, normalize = True)\n",
    "\n",
    "                (FDA_distance_fro, FDA_distance_L1, FDA_distance_inf) = FDA_distance\n",
    "                \n",
    "                FDA_distances_fro.append(FDA_distance_fro.item())\n",
    "                FDA_distances_L1.append(FDA_distance_L1.item())\n",
    "                FDA_distances_inf.append(FDA_distance_inf.item())\n",
    "\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        FDA_distances_fro = np.array(FDA_distances_fro)\n",
    "        FDA_distances_L1 = np.array(FDA_distances_L1)\n",
    "        FDA_distances_inf = np.array(FDA_distances_inf)\n",
    "\n",
    "        result = [epoch, np.mean(FDA_distances_fro), np.var(FDA_distances_fro),\n",
    "                  np.mean(FDA_distances_L1), np.var(FDA_distances_L1), \n",
    "                  np.mean(FDA_distances_inf), np.var(FDA_distances_inf)]\n",
    "\n",
    "        print(\"Mean content FT : {} (mean fro), {} (mean L1), {} (mean inf)\".format(np.mean(FDA_distances_fro), \n",
    "                                                                     np.mean(FDA_distances_L1), np.mean(FDA_distances_inf)))\n",
    "\n",
    "        print(\"Var content FT : {} (var fro), {} (var L1), {} (var inf)\".format(np.var(FDA_distances_fro), \n",
    "                                                                     np.var(FDA_distances_L1), np.var(FDA_distances_inf)))\n",
    "        to_write.append(result)\n",
    "    \n",
    "    str_L = str(L).replace(\".\", \"\")\n",
    "    \n",
    "    with open(head+\"results_content_norm_HFFT_\" + str_L +\"_{}.csv\".format(dataset_mode), \"w\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5452384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
