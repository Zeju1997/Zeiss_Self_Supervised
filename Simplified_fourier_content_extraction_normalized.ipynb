{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d652ea",
   "metadata": {},
   "source": [
    "# Using FDA to evaluate image style transfer\n",
    "\n",
    "This notebook is based upon `FDA: Fourier Domain Adaptation for Semantic Segmentation` from Yanchao Yang.\n",
    "\n",
    "Before starting, the original data and the transferred images should be stored in a known path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36cbed15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeju/miniconda3/envs/vissl_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def high_freq_mutate( amp_src, amp_trg, L=0.1 ):\n",
    "    \"\"\" Change the high frequency from source with the high frequency from target and back transform to image.\n",
    "    \"\"\"\n",
    "    a_src = torch.fft.fftshift( amp_src, dim =(-2, -1) )\n",
    "    a_trg = torch.fft.fftshift( amp_trg, dim =(-2, -1) )\n",
    "\n",
    "    c, h, w = a_src.shape\n",
    "    b = (  np.floor(np.amin((h,w))*L)  ).astype(int)\n",
    "    c_h = np.floor(h/2.0).astype(int)\n",
    "    c_w = np.floor(w/2.0).astype(int)\n",
    "\n",
    "    h1 = c_h-b\n",
    "    h2 = c_h+b+1\n",
    "    w1 = c_w-b\n",
    "    w2 = c_w+b+1\n",
    "\n",
    "    a_src[:,h1:h2,w1:w2] = a_trg[:,h1:h2,w1:w2]\n",
    "    a_src = torch.fft.ifftshift( a_src, dim =(-2, -1) )\n",
    "    return a_src\n",
    "\n",
    "def FDA_source_to_target(src_img, trg_img, L=0.1):\n",
    "    \"\"\" Compute the 2-dimensional FFT of a real array.\n",
    "    \"\"\"\n",
    "\n",
    "    src_img_torch = src_img.clone() #.cpu().numpy()\n",
    "    trg_img_torch = trg_img.clone() #.cpu().numpy()\n",
    "\n",
    "    # get fft of both source and target\n",
    "    fft_src_torch = torch.fft.rfft2( src_img_torch, dim=(-2, -1) )\n",
    "    fft_trg_torch = torch.fft.rfft2( trg_img_torch, dim=(-2, -1) )\n",
    "\n",
    "    # extract amplitude and phase of both ffts\n",
    "    amp_src, pha_src = torch.abs(fft_src_torch), torch.angle(fft_src_torch)\n",
    "    amp_trg, pha_trg = torch.abs(fft_trg_torch), torch.angle(fft_trg_torch)\n",
    "\n",
    "    # mutate the amplitude part of source with target\n",
    "    amp_src_ = high_freq_mutate( amp_src, amp_trg, L=L )\n",
    "\n",
    "    # mutated fft of source\n",
    "    fft_src_ = amp_src_ * torch.exp( 1j * pha_src )\n",
    "\n",
    "    # get the mutated image\n",
    "    src_in_trg = torch.fft.irfft2( fft_src_, dim=(-2, -1) )\n",
    "    #src_in_trg = torch.real(src_in_trg)\n",
    "\n",
    "    return src_in_trg\n",
    "\n",
    "def FDA_distance_torch( src_img, src2trg_img, L=0.1 , normalize = False, display = False):\n",
    "    \"\"\" Calculate FID between feature distribution 1 and feature distribution 2\n",
    "    \"\"\"\n",
    "\n",
    "    src_img_torch = src_img.clone() #.cpu().numpy()\n",
    "    src2trg_img_torch = src2trg_img.clone() #.cpu().numpy()\n",
    "\n",
    "    # get fft of both source and target\n",
    "    fft_src_torch = torch.fft.rfft2( src_img_torch, dim=(-2, -1) )\n",
    "    fft_trg_torch = torch.fft.rfft2( src2trg_img_torch, dim=(-2, -1) )\n",
    "\n",
    "    # extract amplitude and phase of both ffts\n",
    "    amp_src, pha_src = torch.abs(fft_src_torch), torch.angle(fft_src_torch)\n",
    "    amp_trg, pha_trg = torch.abs(fft_trg_torch), torch.angle(fft_trg_torch)\n",
    "\n",
    "    # mutate the amplitude part of source with target\n",
    "    low_freq_part, a_src, a_trg = high_freq_part_torch( amp_src, amp_trg, L=L, normalize = normalize )\n",
    "\n",
    "    low_freq_dist_fro = torch.linalg.norm(torch.flatten(low_freq_part))\n",
    "\n",
    "    low_freq_dist_L1 = torch.linalg.norm(torch.flatten(low_freq_part), ord = 1)\n",
    "\n",
    "    low_freq_dist_inf = torch.linalg.norm(torch.flatten(low_freq_part), ord = float('inf'))\n",
    "    \n",
    "    low_freq_dist = (low_freq_dist_fro, low_freq_dist_L1, low_freq_dist_inf)\n",
    "    \n",
    "    if display:\n",
    "\n",
    "        # mutated fft of source\n",
    "        fft_src_ = a_src * torch.exp( 1j * pha_src )\n",
    "        # mutated fft of source\n",
    "        fft_trg_ = a_trg * torch.exp( 1j * pha_trg )\n",
    "        \n",
    "        low_freq_part_src_ = low_freq_part * torch.exp( 1j * pha_src )\n",
    "        low_freq_part_trg_ = low_freq_part * torch.exp( 1j * pha_trg )\n",
    "        \n",
    "        src_wo_style = torch.fft.irfft2( fft_src_, dim=(-2, -1) )\n",
    "        trg_wo_style = torch.fft.irfft2( fft_trg_, dim=(-2, -1) )\n",
    "        low_freq_part_ifft = torch.fft.irfft2( low_freq_part, dim=(-2, -1) )\n",
    "        low_freq_part_src_ = torch.fft.irfft2( low_freq_part_src_, dim=(-2, -1) )\n",
    "        low_freq_part_trg_ = torch.fft.irfft2( low_freq_part_trg_, dim=(-2, -1) )\n",
    "        \n",
    "        low_freq_tuple = (low_freq_part, low_freq_part_ifft, low_freq_part_src_, low_freq_part_trg_)\n",
    "        \n",
    "        return low_freq_dist, low_freq_tuple, src_wo_style, trg_wo_style\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        return low_freq_dist\n",
    "\n",
    "def high_freq_part_torch( amp_src, amp_trg, L=0.1, normalize = False):\n",
    "    \"\"\" Calculate the normalized difference bewteen source and target image frequency.\n",
    "    \"\"\"\n",
    "    # Shift the zero-frequency component to the center of the spectrum.\n",
    "    a_src = torch.fft.fftshift( amp_src, dim =(-2, -1) )\n",
    "    a_trg = torch.fft.fftshift( amp_trg, dim =(-2, -1) )\n",
    "    \n",
    "    max_src_temp = a_src.max(dim = 1)\n",
    "    max_trg_temp = a_trg.max(dim = 1)\n",
    "    max_src = max_src_temp.values.max(dim = 1)\n",
    "    max_trg = max_trg_temp.values.max(dim = 1)\n",
    "\n",
    "    c, h, w = a_src.shape\n",
    "    b = (  np.floor(np.amin((h,w))*L)  ).astype(int)\n",
    "    c_h = np.floor(h/2.0).astype(int)\n",
    "    c_w = np.floor(w/2.0).astype(int)\n",
    "\n",
    "    h1 = c_h-b\n",
    "    h2 = c_h+b+1\n",
    "    w1 = c_w-b\n",
    "    w2 = c_w+b+1\n",
    "    \n",
    "    a_src[:,h1:h2,w1:w2] = a_src[:,h1:h2,w1:w2] * 0\n",
    "    a_trg[:,h1:h2,w1:w2] = a_trg[:,h1:h2,w1:w2] * 0\n",
    "    \n",
    "    if normalize:\n",
    "        if 0 not in max_src.values:\n",
    "            low_freq_part = torch.div(a_src.permute((1, 2, 0)), max_src.values) - torch.div(a_trg.permute((1, 2, 0)), max_trg.values)\n",
    "        else:\n",
    "            low_freq_part = a_src * 0\n",
    "        low_freq_part = low_freq_part / ((2 * b) * (2 * b))\n",
    "        \n",
    "    else:\n",
    "        low_freq_part = a_src - a_trg\n",
    "        \n",
    "    \n",
    "    a_src = torch.fft.ifftshift( a_src, dim =(-2, -1) )\n",
    "    a_trg = torch.fft.ifftshift( a_trg, dim =(-2, -1) )\n",
    "    \n",
    "    low_freq_part = torch.fft.ifftshift( low_freq_part, dim =(-2, -1) )\n",
    "    \n",
    "    return low_freq_part, a_src, a_trg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33d75967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.cityscapes_dataset import CityscapesDataset\n",
    "from datasets.gta_dataset import GTA5Dataset\n",
    "from datasets.retouch_dataset import Retouch_dataset\n",
    "\n",
    "def create_dataset(dataset_mode, folder_name, split_name, size):\n",
    "    if dataset_mode == \"retouch\":\n",
    "        source_dataset = Retouch_dataset(base_dir=folder_name, list_dir=split_name, size = crop_size)\n",
    "    elif dataset_mode == \"gta5\":\n",
    "        source_dataset = GTA5Dataset(root=folder_name, list_path=split_name, crop_size=size, ignore_label=19)\n",
    "    elif dataset_mode == \"cityscapes\":\n",
    "        source_dataset = CityscapesDataset(root=folder_name, list_path=split_name, crop_size=size, ignore_label=19)\n",
    "    else:\n",
    "        print(\"Unrecognized dataset!\")\n",
    "        sys.exit()\n",
    "        \n",
    "    return source_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8063a7",
   "metadata": {},
   "source": [
    "## Using the custom data in VISSL\n",
    "\n",
    "The original data is saved in the `data` directory. The transferred images are saved in such a way, that they are stored in the `data/generated_images/#epoch` directory (`#epoch` is the number of CycleGAN epoch).\n",
    "\n",
    "**EXAMPLE 1**: download the retouch data set from [retouch-dataset](https://drive.google.com/file/d/1r8pQCoVzEAHdy9wLW_MUkyfgBBFePMPv/view?usp=sharing) and insert it into the `data/real_images` directory. Download the transferred images from [transferred-retouch-images](https://drive.google.com/file/d/1nMcyF-z2yvPBDY70qBsT2Ydg7NUITpmR/view?usp=sharing) and isert the subfolders with the epoch number into the `data/generated_images` directory.\n",
    "\n",
    "**EXAMPLE 2**: download the truncated retouch GTAV data set from [gta5-truncated-dataset](https://drive.google.com/file/d/1R9zmrwAKf03KOq9MSfhdPd6xOVRGEtrY/view?usp=sharing) and insert it into the `data/real_images` directory. Download the transferred images from [transferred-gta5-images](https://drive.google.com/file/d/1SLdGNHDi3LZTHXXNMNFDTmAQibAEjj-x/view?usp=sharing) and isert the subfolders with the epoch number into the `data/generated_images` directory. Note, it also works with the whole data set, one only has to change the `splits/gta5/gta5.txt` to the whole dataset. The truncated version is used due to memory and time efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6d50deb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 3 :\n",
      "Loading from: '/home/zeju/Documents/Zeiss_Self_Supervised/data/generated_images/OCT_new/3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [00:32<00:00, 93.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean content FT : 0.06787624010879274 (mean fro), 9.06347272824496 (mean L1), 0.011698037359110458 (mean inf)\n",
      "Var content FT : 0.0003679565172793263 (var fro), 1.0601259926331206 (var L1), 2.9892104834947856e-05 (var inf)\n",
      "Starting epoch 12 :\n",
      "Loading from: '/home/zeju/Documents/Zeiss_Self_Supervised/data/generated_images/OCT_new/12'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [00:32<00:00, 95.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean content FT : 0.07681800747741363 (mean fro), 8.70493844524026 (mean L1), 0.01619822119391756 (mean inf)\n",
      "Var content FT : 0.0006075895454700128 (var fro), 1.5175142058558773 (var L1), 5.7841632768268003e-05 (var inf)\n",
      "Starting epoch 15 :\n",
      "Loading from: '/home/zeju/Documents/Zeiss_Self_Supervised/data/generated_images/OCT_new/15'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [00:33<00:00, 92.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean content FT : 0.06886866446014513 (mean fro), 8.86506050142149 (mean L1), 0.011183873062009297 (mean inf)\n",
      "Var content FT : 0.0005046855508523822 (var fro), 1.6243900158309474 (var L1), 3.5880192666415044e-05 (var inf)\n",
      "Starting epoch 18 :\n",
      "Loading from: '/home/zeju/Documents/Zeiss_Self_Supervised/data/generated_images/OCT_new/18'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [00:33<00:00, 92.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean content FT : 0.07480982354294004 (mean fro), 8.185165011168769 (mean L1), 0.016372884328120563 (mean inf)\n",
      "Var content FT : 0.0007457351358228135 (var fro), 1.7292686174075793 (var L1), 7.345235986229524e-05 (var inf)\n",
      "Starting epoch 21 :\n",
      "Loading from: '/home/zeju/Documents/Zeiss_Self_Supervised/data/generated_images/OCT_new/21'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [00:33<00:00, 91.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean content FT : 0.07547928482866458 (mean fro), 9.076687966783842 (mean L1), 0.014487132174357006 (mean inf)\n",
      "Var content FT : 0.0004865532010478301 (var fro), 1.6648424304485099 (var L1), 4.504819816128137e-05 (var inf)\n",
      "Starting epoch 24 :\n",
      "Loading from: '/home/zeju/Documents/Zeiss_Self_Supervised/data/generated_images/OCT_new/24'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [00:33<00:00, 90.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean content FT : 0.0789804838859709 (mean fro), 9.905114601987103 (mean L1), 0.015266536481779744 (mean inf)\n",
      "Var content FT : 0.0005895168139869911 (var fro), 1.5073230193015157 (var L1), 5.113223042472342e-05 (var inf)\n",
      "Starting epoch 27 :\n",
      "Loading from: '/home/zeju/Documents/Zeiss_Self_Supervised/data/generated_images/OCT_new/27'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [00:34<00:00, 89.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean content FT : 0.07794486243316594 (mean fro), 8.963636408559978 (mean L1), 0.016056491546957357 (mean inf)\n",
      "Var content FT : 0.0008772217034051315 (var fro), 2.119382445946462 (var L1), 6.935879316783008e-05 (var inf)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "opt = argparse.ArgumentParser()\n",
    "opt.dataset_mode = \"retouch\"\n",
    "opt.method = \"jigsaw\"\n",
    "opt.real_dir = os.path.join(os.getcwd(), \"data/real_images/retouch-dataset\")\n",
    "opt.fake_dir = os.path.join(os.getcwd(), \"data/generated_images/OCT_new\")\n",
    "opt.load_epoch = 0\n",
    "\n",
    "opt.data_list = os.path.join(os.getcwd(), \"splits/cirrus_samples.txt\")\n",
    "\n",
    "opt.crop_size = (512, 512)\n",
    "\n",
    "opt.num_threads = 0  \n",
    "opt.batch_size = 1 \n",
    "opt.no_flip = True  \n",
    "opt.display_id = -1\n",
    "\n",
    "transferred_images_dir = os.path.join(os.getcwd(), opt.fake_dir)\n",
    "epochs = [int(f) for f in os.listdir(transferred_images_dir) if os.path.isdir(os.path.join(transferred_images_dir, f))]\n",
    "epochs.sort()\n",
    "\n",
    "head = os.path.join(os.getcwd(), \"results\")\n",
    "\n",
    "if not os.path.exists(head):\n",
    "    os.makedirs(head)\n",
    "\n",
    "source_dataset = create_dataset(dataset_mode, opt.real_dir, opt.data_list, opt.crop_size)\n",
    "source_loader = DataLoader(source_dataset, batch_size=opt.batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda:{}'.format(0))\n",
    "\n",
    "for L in [0.01]:\n",
    "#for L in [0.05]:\n",
    "\n",
    "    to_write = []\n",
    "    title = [\"epoch\", \"mean fro\", \"var fro\", \"mean L1\", \"var L1\", \"mean inf\", \"var inf\"]\n",
    "    to_write.append(title)\n",
    "\n",
    "    for epoch in epochs:\n",
    "        print(\"Starting epoch {} :\".format(epoch))\n",
    "        folder_name = os.path.join(opt.fake_dir, \"{}\".format(epoch))\n",
    "        print(\"Loading from: '\" + folder_name + \"'\")\n",
    "        source2target_dataset = create_dataset(dataset_mode, folder_name, opt.data_list, opt.crop_size)\n",
    "        source2target_loader = DataLoader(source2target_dataset, batch_size=opt.batch_size, shuffle=False)\n",
    "        source2target_loader_iter = iter(source2target_loader)\n",
    "        FDA_distances_fro = []\n",
    "        FDA_distances_L1 = []\n",
    "        FDA_distances_inf = []\n",
    "\n",
    "        for i, data in enumerate(tqdm(source_loader)):\n",
    "            if i < batch:\n",
    "                source = data[\"image\"].cuda().to(device)\n",
    "                source_img = source[0]\n",
    "\n",
    "                source2target = source2target_loader_iter.next()[\"image\"].cuda().to(device)\n",
    "                source2target_img = source2target[0]\n",
    "\n",
    "                FDA_distance = FDA_distance_torch(src_img = source_img, \n",
    "                                                  src2trg_img = source2target_img, \n",
    "                                                  L = L, normalize = True)\n",
    "\n",
    "                (FDA_distance_fro, FDA_distance_L1, FDA_distance_inf) = FDA_distance\n",
    "                \n",
    "                FDA_distances_fro.append(FDA_distance_fro.item())\n",
    "                FDA_distances_L1.append(FDA_distance_L1.item())\n",
    "                FDA_distances_inf.append(FDA_distance_inf.item())\n",
    "\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        FDA_distances_fro = np.array(FDA_distances_fro)\n",
    "        FDA_distances_L1 = np.array(FDA_distances_L1)\n",
    "        FDA_distances_inf = np.array(FDA_distances_inf)\n",
    "\n",
    "        result = [epoch, np.mean(FDA_distances_fro), np.var(FDA_distances_fro),\n",
    "                  np.mean(FDA_distances_L1), np.var(FDA_distances_L1), \n",
    "                  np.mean(FDA_distances_inf), np.var(FDA_distances_inf)]\n",
    "\n",
    "        print(\"Mean content FT : {} (mean fro), {} (mean L1), {} (mean inf)\".format(np.mean(FDA_distances_fro), \n",
    "                                                                     np.mean(FDA_distances_L1), np.mean(FDA_distances_inf)))\n",
    "\n",
    "        print(\"Var content FT : {} (var fro), {} (var L1), {} (var inf)\".format(np.var(FDA_distances_fro), \n",
    "                                                                     np.var(FDA_distances_L1), np.var(FDA_distances_inf)))\n",
    "        to_write.append(result)\n",
    "    \n",
    "    str_L = str(L).replace(\".\", \"\")\n",
    "    \n",
    "    with open(head+\"results_content_norm_HFFT_\" + str_L +\"_{}.csv\".format(opt.dataset_mode), \"w\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5452384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
