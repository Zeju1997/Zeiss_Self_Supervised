{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/facebookresearch/vissl/blob/v0.1.6/tutorials/Feature_Extraction_V0_1_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1ndZ6XwI7MYA"
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XzxTZfKwFNo"
   },
   "source": [
    "# Feature Extraction\n",
    "\n",
    "In this tutorial, we look at a simple example of how to use VISSL to extract features after finished training the vissl moddels.\n",
    "\n",
    "**EXAMPLE 1**: Download the pre-trained [jigsaw-retouch](https://drive.google.com/file/d/159SgjqklmLHWpEQNq14i_gJk0NDhyAHE/view?usp=sharing) to the `root` directory and rename it to `checkpoints_jigsaw_retouch`.\n",
    "\n",
    "**EXAMPLE 2**: Download the pre-trained [jigsaw-cityscapes](https://drive.google.com/file/d/1Af710oLe_n1h4RMMnhdbxWQWiDCJx68j/view?usp=sharing) to the `root` directory and rename it to `checkpoints_jigsaw_cityscapes`.\n",
    "\n",
    "VISSL should be successfuly installed by now and all the dependencies should be available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Np6atgoOTPrA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeju/miniconda3/envs/vissl_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import vissl\n",
    "import tensorboard\n",
    "import apex\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the custom data in VISSL\n",
    "\n",
    "The original data is saved in the `data` directory. The transferred images are saved in such a way, that they are stored in the `data/generated_images/#epoch` directory (`#epoch` is the number of CycleGAN epoch).\n",
    "\n",
    "**EXAMPLE 1**: download the retouch data set from [retouch-dataset](https://drive.google.com/file/d/1r8pQCoVzEAHdy9wLW_MUkyfgBBFePMPv/view?usp=sharing) and insert it into the `data/real_images` directory. Download the transferred images from [transferred-retouch-images](https://drive.google.com/file/d/1nMcyF-z2yvPBDY70qBsT2Ydg7NUITpmR/view?usp=sharing) and isert the subfolders with the epoch number into the `data/generated_images` directory.\n",
    "\n",
    "**EXAMPLE 2**: download the truncated retouch GTAV data set from [gta5-truncated-dataset](https://drive.google.com/file/d/1R9zmrwAKf03KOq9MSfhdPd6xOVRGEtrY/view?usp=sharing) and insert it into the `data/real_images` directory. Download the transferred images from [transferred-gta5-images](https://drive.google.com/file/d/1SLdGNHDi3LZTHXXNMNFDTmAQibAEjj-x/view?usp=sharing) and isert the subfolders with the epoch number into the `data/generated_images` directory. Note, it also works with the whole data set, one only has to change the `splits/gta5.txt` to the whole dataset. The truncated version is used due to memory and time efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "** fvcore version of PathManager will be deprecated soon. **\n",
      "** Please migrate to the version in iopath repo. **\n",
      "https://github.com/facebookresearch/iopath \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from vissl.utils.hydra_config import AttrDict\n",
    "from vissl.models import build_model\n",
    "from classy_vision.generic.util import load_checkpoint\n",
    "from vissl.utils.checkpoint import init_model_from_consolidated_weights\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import argparse\n",
    "\n",
    "from datasets import create_dataset\n",
    "\n",
    "import sys, os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import linalg\n",
    "import torch\n",
    "\n",
    "from datasets import BaseDataset\n",
    "\n",
    "from datasets.cityscapes_dataset import CityscapesDataset\n",
    "from datasets.gta_dataset import GTA5Dataset\n",
    "from datasets.retouch_dataset import Retouch_dataset\n",
    "\n",
    "\n",
    "def calculate_fid(feat1, feat2):\n",
    "    \"\"\" Calculate FID between feature distribution 1 and feature distribution 2\n",
    "    Args:\n",
    "        feat1: np.array, shape: (N, 2048), dtype: torch.float32 in range 0 - 1\n",
    "        feat2: np.array, shape: (N, 2048), dtype: torch.float32 in range 0 - 1\n",
    "    Returns:\n",
    "        FID (scalar)\n",
    "    \"\"\"\n",
    "    mu1, sigma1 = calculate_activation_statistics(feat1)\n",
    "    mu2, sigma2 = calculate_activation_statistics(feat2)\n",
    "    fid = calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\n",
    "\n",
    "    return fid\n",
    "\n",
    "\n",
    "def calculate_activation_statistics(feat):\n",
    "    \"\"\"Calculates the statistics used by FID\n",
    "    Args:\n",
    "        feat: torch.tensor, shape: (N, 2048), dtype: torch.float32 in range 0 - 1\n",
    "    Returns:\n",
    "        mu:     mean over all activations from the last pool layer of the inception model\n",
    "        sigma:  covariance matrix over all activations from the last pool layer\n",
    "                of the inception model.\n",
    "    \"\"\"\n",
    "\n",
    "    feat_np = feat.cpu().detach().numpy()\n",
    "    mu = np.mean(feat_np, axis=0) # (2048, 0)\n",
    "    sigma = np.cov(feat_np, rowvar=False) # (2048, 2048)\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "# Modified from: https://github.com/bioinf-jku/TTUR/blob/master/fid.py\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\n",
    "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
    "    and X_2 ~ N(mu_2, C_2) is\n",
    "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "\n",
    "    Stable version by Dougal J. Sutherland.\n",
    "    Params:\n",
    "    -- mu1 : Numpy array containing the activations of the pool_3 layer of the\n",
    "             inception net ( like returned by the function 'get_predictions')\n",
    "             for generated samples.\n",
    "    -- mu2   : The sample mean over activations of the pool_3 layer, precalcualted\n",
    "               on an representive data set.\n",
    "    -- sigma1: The covariance matrix over activations of the pool_3 layer for\n",
    "               generated samples.\n",
    "    -- sigma2: The covariance matrix over activations of the pool_3 layer,\n",
    "               precalcualted on an representive data set.\n",
    "    Returns:\n",
    "    --   : The Frechet Distance.\n",
    "    \"\"\"\n",
    "\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\n",
    "    assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "    # product might be almost singular\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = \"fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps\n",
    "        print(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    # numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError(\"Imaginary component {}\".format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
    "\n",
    "\n",
    "def create_dataset(dataset_mode, folder_name, sample_list, size):\n",
    "    \"\"\"\n",
    "    -- dataset_mode : specify the data set to be load\n",
    "    -- folder_name  : the path to the directory, which the data is stored\n",
    "    -- sample_list  : path to the .txt split file \n",
    "    -- size         : data crop size\n",
    "    Returns:\n",
    "    --   : dataset\n",
    "    \"\"\"\n",
    "    if dataset_mode == \"retouch\":\n",
    "        dataset = Retouch_dataset(base_dir=folder_name, list_dir=sample_list, size = size)\n",
    "    elif dataset_mode == \"gta5\":\n",
    "        dataset = GTA5Dataset(root=folder_name, list_path=sample_list, crop_size=size, ignore_label=19)\n",
    "    elif dataset_mode == \"cityscapes\":\n",
    "        dataset = CityscapesDataset(root=folder_name, list_path=sample_list, crop_size=size, ignore_label=19)\n",
    "    else:\n",
    "        print(\"Unrecognized dataset!\")\n",
    "        sys.exit()\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def run_eval(opt):\n",
    "    real_dataset = create_dataset(opt.real_dataset_mode, opt.real_dir, opt.real_list, opt.crop_size)\n",
    "    fake_dataset = create_dataset(opt.fake_dataset_mode, opt.fake_dir, opt.fake_list, opt.crop_size)\n",
    "\n",
    "    real_loader = torch.utils.data.DataLoader(real_dataset,\n",
    "                                         batch_size=opt.batch_size,\n",
    "                                         shuffle=opt.shuffle,\n",
    "                                         num_workers=opt.num_threads,\n",
    "                                         pin_memory=True,\n",
    "                                         drop_last=False)\n",
    "\n",
    "    fake_loader = torch.utils.data.DataLoader(fake_dataset,\n",
    "                                         batch_size=opt.batch_size,\n",
    "                                         shuffle=opt.shuffle,\n",
    "                                         num_workers=opt.num_threads,\n",
    "                                         pin_memory=True,\n",
    "                                         drop_last=False)\n",
    "    \n",
    "    model.cuda()\n",
    "    # model.eval()\n",
    "    feature_fake = 0\n",
    "    for idx, input in enumerate(tqdm(fake_loader)):\n",
    "        features = model(input['image'].cuda())\n",
    "        feat = torch.flatten(features[0], start_dim=1)\n",
    "        if idx == 0:\n",
    "            feature_fake = torch.zeros((len(fake_loader), feat.shape[1]))\n",
    "            feature_fake[idx, :] = feat\n",
    "        else:\n",
    "            feature_fake[idx, :] = feat\n",
    "\n",
    "    feature_real = 0\n",
    "    if not os.path.exists(\"real.pt\"):\n",
    "        for idx, input in enumerate(tqdm(real_loader)):\n",
    "            features = model(input['image'].cuda())\n",
    "            feat = torch.flatten(features[0], start_dim=1)\n",
    "            if idx == 0:\n",
    "                feature_real = torch.zeros((len(real_loader), feat.shape[1]))\n",
    "                feature_real[idx, :] = feat\n",
    "                # feature_target = feat\n",
    "            else:\n",
    "                feature_real[idx, :] = feat\n",
    "                # feature_target = torch.cat((feature_target, feat), 0)\n",
    "            \n",
    "        torch.save(feature_real, 'real.pt')\n",
    "    else:\n",
    "        feature_real = torch.load('real.pt')\n",
    "\n",
    "    fid = calculate_fid(feature_fake, feature_real)\n",
    "    print(\"Epoch {}:\".format(opt.load_epoch), \"score\", fid)\n",
    "\n",
    "    csv_path = os.path.join(os.getcwd(), \"results\", \"self_supervised_results_{}_{}.csv\".format(opt.real_dataset_mode, opt.method))\n",
    "   \n",
    "    if os.path.isfile(csv_path):\n",
    "        x = []\n",
    "        value = []\n",
    "        with open(csv_path, 'r') as csvfile:\n",
    "            lines = csv.reader(csvfile, delimiter=',')\n",
    "            for idx, row in enumerate(lines):\n",
    "                if idx != 0:\n",
    "                    x.append(row[0])\n",
    "                    value.append(row[1])\n",
    "        x.append(opt.load_epoch)\n",
    "        value.append(fid)\n",
    "        x_np = np.asarray(x).astype(int)\n",
    "        value_np = np.asarray(value).astype(float)\n",
    "\n",
    "    to_write = []\n",
    "    to_write.append([\"epoch\", opt.method])\n",
    "\n",
    "    if os.path.isfile(csv_path):\n",
    "        for epoch in range(len(x_np)):\n",
    "            result = [x_np[epoch], value_np[epoch]]\n",
    "            to_write.append(result)\n",
    "    else:\n",
    "        result = [opt.load_epoch, fid]\n",
    "        to_write.append(result)\n",
    "\n",
    "    with open(csv_path, \"w\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(to_write)\n",
    "DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the correct config file:\n",
    "```\n",
    "jigsaw_custom_retouch\n",
    "jigsaw_custom_cityscapes\n",
    "jigsaw_custom_mnist\n",
    "jigsaw_custom_synthia\n",
    "rotnet_custom_retouch\n",
    "rotnet_custom_cityscapes\n",
    "rotnet_custom_mnist\n",
    "rotnet_custom_synthia\n",
    "```\n",
    "Extract features and calculating the FID score using pre-trained self-supervised models. \n",
    "\n",
    "**EXAMPLE 1**: evaluate the transformation from GTAV to Cityscapes dataset.\n",
    "Specify the `real_dataset_mode` as `cityscapes` and the `fake_dataset_mode` as `gta5`(transferred images). Specify the path to the original target data `real_dir` and the transfearred source data `fake_base_dir` and `fake_dir`. Split files are stored in the `splits` directory. The models are saved in the directory with pattern like `checkpoints_jigsaw_retouch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run eval epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [00:58<00:00, 52.13it/s]\n",
      "100%|███████████████████████████████████████| 1176/1176 [00:24<00:00, 47.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: score 2.2903170215644884\n",
      "run eval epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:11<00:00, 43.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: score 1.8162163244578187\n",
      "run eval epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:12<00:00, 42.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: score 2.0907976446924845\n",
      "run eval epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:12<00:00, 42.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: score 1.2546606785351386\n",
      "run eval epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:14<00:00, 41.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: score 1.2819428139141342\n",
      "run eval epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:16<00:00, 40.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: score 1.0108681780766666\n",
      "run eval epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:14<00:00, 41.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: score 2.0651648416206108\n",
      "run eval epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:14<00:00, 41.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: score 1.1955209744112185\n",
      "run eval epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:14<00:00, 41.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: score 0.8197001864983129\n",
      "run eval epoch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:13<00:00, 41.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: score 0.6926377153062795\n",
      "run eval epoch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:14<00:00, 41.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: score 0.7518902664435654\n",
      "run eval epoch 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:14<00:00, 41.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: score 0.6368103378635217\n",
      "run eval epoch 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:13<00:00, 41.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: score 0.6442715109059884\n",
      "run eval epoch 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:12<00:00, 42.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: score 0.9172350380175649\n",
      "run eval epoch 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:13<00:00, 41.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: score 0.5461726832980087\n",
      "run eval epoch 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:13<00:00, 41.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: score 0.8747915805891111\n",
      "run eval epoch 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:14<00:00, 41.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: score 0.5397331149413782\n",
      "run eval epoch 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:14<00:00, 41.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: score 0.6009107697813469\n",
      "run eval epoch 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:12<00:00, 42.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: score 0.6787478353544927\n",
      "run eval epoch 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:14<00:00, 41.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63: score 0.6012174653602775\n",
      "run eval epoch 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:15<00:00, 40.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: score 0.6532430636020683\n",
      "run eval epoch 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:14<00:00, 41.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69: score 0.738707159725112\n",
      "run eval epoch 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:15<00:00, 40.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72: score 0.666600434700249\n",
      "run eval epoch 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:15<00:00, 40.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: score 0.7800928812733101\n",
      "run eval epoch 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:12<00:00, 42.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78: score 0.5301610752178698\n",
      "run eval epoch 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|████████████████████▎                  | 1603/3072 [00:39<00:35, 41.02it/s]"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    opt = argparse.ArgumentParser()\n",
    "    opt.real_dataset_mode = \"retouch\"\n",
    "    opt.fake_dataset_mode = \"retouch\"\n",
    "    opt.method = \"jigsaw\"\n",
    "    opt.real_dir = os.path.join(os.getcwd(), \"datasets/real_images/retouch-dataset\")\n",
    "    opt.fake_base_dir = os.path.join(os.getcwd(), \"datasets/generated_images/OCT_new\")\n",
    "    opt.fake_dir = os.path.join(os.getcwd(), \"datasets/generated_images/OCT_new\")\n",
    "    opt.load_epoch = 0\n",
    "    opt.model_phase = 90\n",
    "    \n",
    "    opt.real_list = os.path.join(os.getcwd(), \"splits/spectralis_samples.txt\")\n",
    "    opt.fake_list = os.path.join(os.getcwd(), \"splits/cirrus_samples.txt\")\n",
    "    \n",
    "    opt.crop_size= (512, 512)\n",
    "    \n",
    "    opt.num_threads = 0  \n",
    "    opt.batch_size = 1 \n",
    "    opt.shuffle = True  \n",
    "    opt.no_flip = True  \n",
    "    opt.display_id = -1\n",
    "    \n",
    "    config = OmegaConf.load(\"configs/config/pretrain/{}/{}_custom_{}.yaml\".format(opt.method, opt.method, opt.real_dataset_mode))\n",
    "\n",
    "    default_config = OmegaConf.load(\"vissl/config/defaults.yaml\")\n",
    "    cfg = OmegaConf.merge(default_config, config)\n",
    "\n",
    "    cfg = AttrDict(cfg)\n",
    "    cfg.config.MODEL._MODEL_INIT_SEED = 0\n",
    "    cfg.config.MODEL.WEIGHTS_INIT.PARAMS_FILE = \"./checkpoints_{}_{}/model_phase{}.torch\".format(opt.method, opt.real_dataset_mode, opt.model_phase)\n",
    "    cfg.config.MODEL.FEATURE_EVAL_SETTINGS.EVAL_MODE_ON = True\n",
    "    cfg.config.MODEL.FEATURE_EVAL_SETTINGS.FREEZE_TRUNK_ONLY = True\n",
    "    cfg.config.MODEL.FEATURE_EVAL_SETTINGS.EXTRACT_TRUNK_FEATURES_ONLY = True\n",
    "    cfg.config.MODEL.FEATURE_EVAL_SETTINGS.SHOULD_FLATTEN_FEATS = False\n",
    "    cfg.config.MODEL.FEATURE_EVAL_SETTINGS.LINEAR_EVAL_FEAT_POOL_OPS_MAP = [[\"res5avg\", [\"Identity\", []]]]\n",
    "\n",
    "    model = build_model(cfg.config.MODEL, cfg.config.OPTIMIZER)\n",
    "    weights = load_checkpoint(checkpoint_path=cfg.config.MODEL.WEIGHTS_INIT.PARAMS_FILE)\n",
    "    model.cuda()\n",
    "\n",
    "    init_model_from_consolidated_weights(\n",
    "        config=cfg.config,\n",
    "        model=model,\n",
    "        state_dict=weights,\n",
    "        state_dict_key_name=\"classy_state_dict\",\n",
    "        skip_layers=[],  # Use this if you do not want to load all layers\n",
    "    )\n",
    "\n",
    "    head = \"results/\"\n",
    "\n",
    "    if not os.path.exists(head):\n",
    "        os.makedirs(head)\n",
    "\n",
    "    transferred_images_dir = opt.fake_dir\n",
    "    epochs = [int(f) for f in os.listdir(transferred_images_dir) if os.path.isdir(os.path.join(transferred_images_dir, f))]\n",
    "    epochs.sort()\n",
    "    \n",
    "    # target feature distribtuion\n",
    "    if os.path.exists(\"real.pt\"):\n",
    "        os.remove(\"real.pt\")\n",
    "\n",
    "    for epoch in epochs:\n",
    "        print(\"run eval epoch {}\".format(epoch))\n",
    "        opt.fake_dir = os.path.join(opt.fake_base_dir, \"{}\".format(epoch))\n",
    "        opt.load_epoch = int(epoch)\n",
    "        run_eval(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Feature Extraction V0.1.6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
