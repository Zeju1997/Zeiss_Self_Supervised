{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/facebookresearch/vissl/blob/v0.1.6/tutorials/Feature_Extraction_V0_1_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1ndZ6XwI7MYA"
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XzxTZfKwFNo"
   },
   "source": [
    "# Feature Extraction\n",
    "\n",
    "In this tutorial, we look at a simple example of how to use VISSL to extract features after finished training the vissl moddels.\n",
    "\n",
    "**EXAMPLE 1**: Download the pre-trained [jigsaw-retouch](https://drive.google.com/file/d/159SgjqklmLHWpEQNq14i_gJk0NDhyAHE/view?usp=sharing) to the `root` directory and rename it to `checkpoints`.\n",
    "\n",
    "**EXAMPLE 2**: Download the pre-trained [jigsaw-cityscapes](https://drive.google.com/file/d/1Af710oLe_n1h4RMMnhdbxWQWiDCJx68j/view?usp=sharing) to the `root` directory and rename it to `checkpoints`.\n",
    "\n",
    "VISSL should be successfuly installed by now and all the dependencies should be available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Np6atgoOTPrA"
   },
   "outputs": [],
   "source": [
    "import vissl\n",
    "import tensorboard\n",
    "import apex\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the custom data in VISSL\n",
    "\n",
    "The original data is saved in the `data` directory. The transferred images are saved in such a way, that they are stored in the `data/transferred/#epoch` directory (`#epoch` is the number of CycleGAN epoch).\n",
    "\n",
    "**EXAMPLE 1**: download the retouch data set from [retouch-dataset](https://drive.google.com/file/d/1r8pQCoVzEAHdy9wLW_MUkyfgBBFePMPv/view?usp=sharing) and insert it into the `data` directory. Download the transferred images from [transferred-retouch-images](https://drive.google.com/file/d/1nMcyF-z2yvPBDY70qBsT2Ydg7NUITpmR/view?usp=sharing) and isert the subfolders with the epoch number into the `data/transferred` directory.\n",
    "\n",
    "**EXAMPLE 2**: download the truncated retouch GTAV data set from [gta5-truncated-dataset](https://drive.google.com/file/d/1R9zmrwAKf03KOq9MSfhdPd6xOVRGEtrY/view?usp=sharing) and insert it into the `data` directory. Download the transferred images from [transferred-gta5-images](https://drive.google.com/file/d/1SLdGNHDi3LZTHXXNMNFDTmAQibAEjj-x/view?usp=sharing) and isert the subfolders with the epoch number into the `data/transferred` directory. Note, it also works with the whole data set, one only has to change the `splits/gta5/gta5.txt` to the whole dataset. The truncated version is used due to memory and time efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3738/2587059533.py:6: DeprecationWarning: Please use `zoom` from the `scipy.ndimage` namespace, the `scipy.ndimage.interpolation` namespace is deprecated.\n",
      "  from scipy.ndimage.interpolation import zoom\n"
     ]
    }
   ],
   "source": [
    "import skimage.io as io\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from torch.utils.data import Dataset\n",
    "# from medpy.io import load\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image  # using pillow-simd for increased speed\n",
    "\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning\n",
    "    # (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert(\"L\")\n",
    "\n",
    "class Retouch_dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 base_dir,\n",
    "                 list_dir,\n",
    "                 size=(512, 512),\n",
    "                 split='train',\n",
    "                 is_train=False,\n",
    "                 transform=None,\n",
    "                 ext='.png'):\n",
    "        self.transform = transform  # using transform in torch!\n",
    "        self.split = split\n",
    "        if split == '':\n",
    "            self.sample_list = open(list_dir).readlines()\n",
    "        else:\n",
    "            self.sample_list = open(os.path.join(list_dir,\n",
    "                                                 self.split + '.txt')).readlines()\n",
    "        self.data_dir = base_dir\n",
    "        self.loader = pil_loader\n",
    "        self.to_tensor = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size),\n",
    "                transforms.ToTensor(),\n",
    "                #transforms.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "            ])\n",
    "\n",
    "        self.is_train = is_train\n",
    "        self.transform = transform\n",
    "        self.ext = ext\n",
    "\n",
    "    def augment(self, data, label):\n",
    "        data_label = torch.cat((data, label), dim=0)\n",
    "        data_label_aug = self.transform(data_label)\n",
    "        data_aug = data_label_aug[0, :, :].unsqueeze(0)\n",
    "        label_aug = data_label_aug[1, :, :].unsqueeze(0)\n",
    "        return data_aug, label_aug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_name = self.sample_list[idx].strip('\\n')\n",
    "\n",
    "        vendor = sample_name.split(' ')[0]\n",
    "        slice_name = sample_name.split(' ')[1]\n",
    "        slice_idx = sample_name.split(' ')[2].zfill(3)\n",
    "\n",
    "        data_path = os.path.join(self.data_dir,\n",
    "                                 vendor,\n",
    "                                 slice_name,\n",
    "                                 'images',\n",
    "                                 slice_idx + self.ext)\n",
    "        '''\n",
    "\n",
    "        label_path = os.path.join(self.data_dir,\n",
    "                                  vendor,\n",
    "                                  slice_name,\n",
    "                                  'labels',\n",
    "                                  slice_idx + '.npy')\n",
    "\n",
    "        label = torch.from_numpy(np.load(label_path))\n",
    "        label_idx = torch.argmax(label, dim=0, keepdim=True)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        data = self.to_tensor(self.loader(data_path))\n",
    "        transform_avaliable = self.transform is not None and self.is_train\n",
    "        do_aug = transform_avaliable and random.random() > 0.5\n",
    "\n",
    "        if do_aug:\n",
    "            # data, label_idx = self.augment(data, label_idx)\n",
    "            data = self.augment(data)\n",
    "\n",
    "        # label_idx = label_idx.squeeze(0).long()\n",
    "\n",
    "        sample = {'image': data.repeat(3, 1, 1),\n",
    "                  'case_name': sample_name}\n",
    "        # print((label_idx==0).sum()/512**2)\n",
    "        return sample\n",
    "\n",
    "# Test Unit\n",
    "# flip = transforms.RandomHorizontalFlip(p=0.5)\n",
    "# base_dir = 'Retouch-dataset_test/pre_processed/'\n",
    "# list_dir = ''\n",
    "# dataset = Retouch_dataset(base_dir, list_dir, transform=flip)\n",
    "# l = dataset[3]['label']\n",
    "# d = dataset[3]['image']\n",
    "#\n",
    "# print(l.shape, d.shape)\n",
    "#\n",
    "# img = d.permute(1, 2, 0).numpy()\n",
    "# print((img[:, :, 0] == img[:, :, 2]).all())\n",
    "# print(dataset[3]['case_name'])\n",
    "# plt.figure()\n",
    "# plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gta5 dataset (source)\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "'''\n",
    "labels = [\n",
    "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
    "    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
    "    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
    "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
    "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
    "    Label(  'parking'              ,  9 ,      255 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
    "    Label(  'rail track'           , 10 ,      255 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
    "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
    "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
    "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
    "    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
    "    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
    "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
    "    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
    "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
    "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
    "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
    "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
    "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
    "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
    "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
    "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
    "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
    "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
    "    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
    "    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
    "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
    "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
    "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
    "    Label(  'license plate'        , -1 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
    "]\n",
    "'''\n",
    "\n",
    "class GTA5Dataset(data.Dataset):\n",
    "    def __init__(self, root, list_path, max_iters=None, crop_size=(256, 256), mean=(128, 128, 128), ignore_label=255):\n",
    "        self.root = root\n",
    "        self.list_path = list_path\n",
    "        self.crop_size = crop_size\n",
    "        self.ignore_label = ignore_label\n",
    "        self.mean = mean\n",
    "        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n",
    "        if not max_iters==None:\n",
    "            self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))\n",
    "        self.files = []\n",
    "\n",
    "        self.id_to_trainid = {7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,\n",
    "                              19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12,\n",
    "                              26: 13, 27: 14, 28: 15, 31: 16, 32: 17, 33: 18}\n",
    "\n",
    "        self.id2label = {-1: ignore_label, 0: ignore_label, 1: ignore_label, 2: ignore_label,\n",
    "            3: ignore_label, 4: ignore_label, 5: ignore_label, 6: ignore_label,\n",
    "            7: 0, 8: 1, 9: ignore_label, 10: ignore_label, 11: 2, 12: 3, 13: 4,\n",
    "            14: ignore_label, 15: ignore_label, 16: ignore_label, 17: 5,\n",
    "            18: ignore_label, 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14,\n",
    "            28: 15, 29: ignore_label, 30: ignore_label, 31: 16, 32: 17, 33: 18}\n",
    "\n",
    "        #self.id_to_trainid = {7: 1, 24: 2, 26: 3} #Road/car/people\n",
    "        self.id_to_trainid = {11: 1, 24: 2, 21: 3} #Building/car/vegetation\n",
    "        #self.ignore_label = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name = self.img_ids[index]\n",
    "        \n",
    "        image = Image.open(osp.join(self.root, \"images/%s\" % name)).convert('RGB')\n",
    "        label = Image.open(osp.join(self.root, \"labels/%s\" % name))\n",
    "        # resize\n",
    "        image = image.resize(self.crop_size, Image.BICUBIC)\n",
    "        label = label.resize(self.crop_size, Image.NEAREST)\n",
    "\n",
    "        image = np.asarray(image, np.float32)\n",
    "        label = np.asarray(label, np.int8)\n",
    "\n",
    "        label_copy = self.ignore_label * np.ones(label.shape, dtype=np.float32)\n",
    "        #for k, v in self.id_to_trainid.items():\n",
    "        for k, v in self.id2label.items():\n",
    "            label_copy[label == k] = v\n",
    "        size = image.shape\n",
    "        #image = image[:, :, ::-1]  # change to BGR\n",
    "        #image -= self.mean\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        sample = {'image': image.copy(),\n",
    "                  'label': label_copy.copy()}\n",
    "\n",
    "        return sample\n",
    "    \n",
    "class GTA5Dataset1(data.Dataset):\n",
    "    def __init__(self, root, list_path, max_iters=None, crop_size=(256, 256), mean=(128, 128, 128), ignore_label=255):\n",
    "        self.root = root\n",
    "        self.list_path = list_path\n",
    "        self.crop_size = crop_size\n",
    "        self.ignore_label = ignore_label\n",
    "        self.mean = mean\n",
    "        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n",
    "        if not max_iters==None:\n",
    "            self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))\n",
    "        self.files = []\n",
    "\n",
    "        self.id_to_trainid = {7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,\n",
    "                              19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12,\n",
    "                              26: 13, 27: 14, 28: 15, 31: 16, 32: 17, 33: 18}\n",
    "\n",
    "        self.id2label = {-1: ignore_label, 0: ignore_label, 1: ignore_label, 2: ignore_label,\n",
    "            3: ignore_label, 4: ignore_label, 5: ignore_label, 6: ignore_label,\n",
    "            7: 0, 8: 1, 9: ignore_label, 10: ignore_label, 11: 2, 12: 3, 13: 4,\n",
    "            14: ignore_label, 15: ignore_label, 16: ignore_label, 17: 5,\n",
    "            18: ignore_label, 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14,\n",
    "            28: 15, 29: ignore_label, 30: ignore_label, 31: 16, 32: 17, 33: 18}\n",
    "\n",
    "        #self.id_to_trainid = {7: 1, 24: 2, 26: 3} #Road/car/people\n",
    "        self.id_to_trainid = {11: 1, 24: 2, 21: 3} #Building/car/vegetation\n",
    "        #self.ignore_label = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name = self.img_ids[index]\n",
    "                \n",
    "        name = name[:-4] + \".jpg\"\n",
    "        \n",
    "        image = Image.open(osp.join(self.root, \"images/%s\" % name)).convert('RGB')\n",
    "        \n",
    "        name = name[:-4] + \".png\"\n",
    "        label = Image.open(osp.join(self.root, \"labels/%s\" % name))\n",
    "        # resize\n",
    "        image = image.resize(self.crop_size, Image.BICUBIC)\n",
    "        label = label.resize(self.crop_size, Image.NEAREST)\n",
    "\n",
    "        image = np.asarray(image, np.float32)\n",
    "        label = np.asarray(label, np.int8)\n",
    "\n",
    "        label_copy = self.ignore_label * np.ones(label.shape, dtype=np.float32)\n",
    "        #for k, v in self.id_to_trainid.items():\n",
    "        for k, v in self.id2label.items():\n",
    "            label_copy[label == k] = v\n",
    "        size = image.shape\n",
    "        #image = image[:, :, ::-1]  # change to BGR\n",
    "        #image -= self.mean\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        sample = {'image': image.copy(),\n",
    "                  'label': label_copy.copy()}\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cityscapes dataset (target)\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "palette = [128, 64, 128, 244, 35, 232, 70, 70, 70, 102, 102, 156, 190, 153, 153, 153, 153, 153, 250, 170, 30,\n",
    "            220, 220, 0, 107, 142, 35, 152, 251, 152, 70, 130, 180, 220, 20, 60, 255, 0, 0, 0, 0, 142, 0, 0, 70,\n",
    "            0, 60, 100, 0, 80, 100, 0, 0, 230, 119, 11, 32]\n",
    "classes = ['road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light', 'traffic sign',\n",
    "        'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck', 'bus', 'train', 'motorcycle',\n",
    "        'bicycle']\n",
    "\n",
    "    \n",
    "class CityscapesDataset(data.Dataset):\n",
    "    def __init__(self, root, list_path, max_iters=None, crop_size=(256, 256), mean=(128, 128, 128), ignore_label=255):\n",
    "        self.root = root\n",
    "        self.list_path = list_path\n",
    "        self.crop_size = crop_size\n",
    "        self.ignore_label = ignore_label\n",
    "        self.mean = mean\n",
    "        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n",
    "        if not max_iters==None:\n",
    "            self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))\n",
    "        self.files = []\n",
    "        \n",
    "        self.id_to_trainid = {7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,\n",
    "                              19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12,\n",
    "                              26: 13, 27: 14, 28: 15, 31: 16, 32: 17, 33: 18}\n",
    "\n",
    "        self.id2label = {-1: ignore_label, 0: ignore_label, 1: ignore_label, 2: ignore_label,\n",
    "            3: ignore_label, 4: ignore_label, 5: ignore_label, 6: ignore_label,\n",
    "            7: 0, 8: 1, 9: ignore_label, 10: ignore_label, 11: 2, 12: 3, 13: 4,\n",
    "            14: ignore_label, 15: ignore_label, 16: ignore_label, 17: 5,\n",
    "            18: ignore_label, 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14,\n",
    "            28: 15, 29: ignore_label, 30: ignore_label, 31: 16, 32: 17, 33: 18}\n",
    "\n",
    "        #self.id_to_trainid = {7: 1, 24: 2, 26: 3} #Road/car/people\n",
    "        self.id_to_trainid = {11: 1, 24: 2, 21: 3} #Building/car/vegetation\n",
    "        #self.ignore_label = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name = self.img_ids[index]\n",
    "        image_root = osp.join(self.root, 'cityscapes')\n",
    "        label_root = osp.join(self.root, 'gtFine')\n",
    "        image = Image.open(osp.join(image_root, \"%s\" % name)).convert('RGB')\n",
    "        label = Image.open(osp.join(label_root, \"%s\" % name.replace(\"leftImg8bit\", \"gtFine_labelIds\")))\n",
    "        # resize\n",
    "        image = image.resize(self.crop_size, Image.BICUBIC)\n",
    "        label = label.resize(self.crop_size, Image.NEAREST)\n",
    "\n",
    "        image = np.asarray(image, np.float32)\n",
    "        label = np.asarray(label, np.int8)\n",
    "\n",
    "        label_copy = self.ignore_label * np.ones(label.shape, dtype=np.float32)\n",
    "        #for k, v in self.id_to_trainid.items():\n",
    "        for k, v in self.id2label.items():\n",
    "            label_copy[label == k] = v\n",
    "        size = image.shape\n",
    "        #image = image[:, :, ::-1]  # change to BGR\n",
    "        #image -= self.mean\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        sample = {'image': image.copy(),\n",
    "                  'label': label_copy.copy()}\n",
    "\n",
    "        return sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from vissl.utils.hydra_config import AttrDict\n",
    "from vissl.models import build_model\n",
    "from classy_vision.generic.util import load_checkpoint\n",
    "from vissl.utils.checkpoint import init_model_from_consolidated_weights\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import argparse\n",
    "\n",
    "from datasets import create_dataset\n",
    "\n",
    "import sys, os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import linalg\n",
    "import torch\n",
    "\n",
    "\n",
    "def calculate_fid(feat1, feat2):\n",
    "    \"\"\" Calculate FID between images1 and images2\n",
    "    Args:\n",
    "        images1: np.array, shape: (N, H, W, 3), dtype: np.float32 between 0-1 or np.uint8\n",
    "        images2: np.array, shape: (N, H, W, 3), dtype: np.float32 between 0-1 or np.uint8\n",
    "        use_multiprocessing: If multiprocessing should be used to pre-process the images\n",
    "        batch size: batch size used for inception network\n",
    "    Returns:\n",
    "        FID (scalar)\n",
    "    \"\"\"\n",
    "    mu1, sigma1 = calculate_activation_statistics(feat1)\n",
    "    mu2, sigma2 = calculate_activation_statistics(feat2)\n",
    "    fid = calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\n",
    "\n",
    "    return fid\n",
    "\n",
    "\n",
    "def calculate_activation_statistics(feat):\n",
    "    \"\"\"Calculates the statistics used by FID\n",
    "    Args:\n",
    "        images: torch.tensor, shape: (N, 3, H, W), dtype: torch.float32 in range 0 - 1\n",
    "        batch_size: batch size to use to calculate inception scores\n",
    "    Returns:\n",
    "        mu:     mean over all activations from the last pool layer of the inception model\n",
    "        sigma:  covariance matrix over all activations from the last pool layer\n",
    "                of the inception model.\n",
    "    \"\"\"\n",
    "\n",
    "    feat_np = feat.cpu().detach().numpy()\n",
    "    mu = np.mean(feat_np, axis=0) # (2048, 0)\n",
    "    sigma = np.cov(feat_np, rowvar=False) # (2048, 2048)\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "# Modified from: https://github.com/bioinf-jku/TTUR/blob/master/fid.py\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\n",
    "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
    "    and X_2 ~ N(mu_2, C_2) is\n",
    "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "\n",
    "    Stable version by Dougal J. Sutherland.\n",
    "    Params:\n",
    "    -- mu1 : Numpy array containing the activations of the pool_3 layer of the\n",
    "             inception net ( like returned by the function 'get_predictions')\n",
    "             for generated samples.\n",
    "    -- mu2   : The sample mean over activations of the pool_3 layer, precalcualted\n",
    "               on an representive data set.\n",
    "    -- sigma1: The covariance matrix over activations of the pool_3 layer for\n",
    "               generated samples.\n",
    "    -- sigma2: The covariance matrix over activations of the pool_3 layer,\n",
    "               precalcualted on an representive data set.\n",
    "    Returns:\n",
    "    --   : The Frechet Distance.\n",
    "    \"\"\"\n",
    "\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\n",
    "    assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "    # product might be almost singular\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = \"fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps\n",
    "        print(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    # numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError(\"Imaginary component {}\".format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
    "\n",
    "\n",
    "def create_dataset(dataset_mode, folder_name, split_name, split, size):\n",
    "    if dataset_mode == \"retouch\":\n",
    "        source_dataset = Retouch_dataset(base_dir=folder_name, list_dir=split_name, split='', size = crop_size)\n",
    "    elif dataset_mode == \"gta5\":\n",
    "        source_dataset = GTA5Dataset(root=folder_name, list_path=split_name, crop_size=size, ignore_label=19)\n",
    "    elif dataset_mode == \"cityscapes\":\n",
    "        source_dataset = CityscapesDataset(root=folder_name, list_path=split_name, crop_size=size, ignore_label=19)\n",
    "    else:\n",
    "        print(\"Unrecognized dataset!\")\n",
    "        sys.exit()\n",
    "        \n",
    "    return source_dataset\n",
    "\n",
    "\n",
    "def run_eval(epoch):\n",
    "    opt = argparse.ArgumentParser()  # get test options\n",
    "    # hard-code some parameters for test\n",
    "    opt.name = 'full_model'\n",
    "    opt.model = 'cycle_gan'\n",
    "\n",
    "    opt.num_threads = 0  # test code only supports num_threads = 0\n",
    "    opt.batch_size = 1  # test code only supports batch_size = 1\n",
    "    opt.serial_batches = True  # disable data shuffling; comment this line if results on randomly chosen images are needed.\n",
    "    opt.no_flip = True  # no flip; comment this line if results on flipped images are needed.\n",
    "    opt.display_id = -1  # no visdom display; the test code saves the results to a HTML file.\n",
    "\n",
    "    opt.netG = \"unet\"\n",
    "\n",
    "    opt.load_epoch = epoch\n",
    "\n",
    "    opt.base_dir = os.path.join(os.getcwd(), \"data/transferred/{}\".format(opt.load_epoch))\n",
    "    opt.list_dir = os.path.join(os.getcwd(), \"splits\")\n",
    "    source_dataset = Retouch_dataset(base_dir=opt.base_dir,\n",
    "                          list_dir=opt.list_dir,\n",
    "                          split='cirrus_samples',\n",
    "                          is_train=False)\n",
    "\n",
    "    source_loader = torch.utils.data.DataLoader(source_dataset,\n",
    "                                         1,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=opt.num_threads,\n",
    "                                         pin_memory=True,\n",
    "                                         drop_last=False)\n",
    "\n",
    "    opt.base_dir = os.path.join(os.getcwd(), \"data/retouch\")\n",
    "    target_dataset = Retouch_dataset(base_dir=opt.base_dir,\n",
    "                          list_dir=opt.list_dir,\n",
    "                          split='spectralis_samples',\n",
    "                          is_train=False)\n",
    "\n",
    "    target_loader = torch.utils.data.DataLoader(target_dataset,\n",
    "                                         1,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=opt.num_threads,\n",
    "                                         pin_memory=True,\n",
    "                                         drop_last=False)\n",
    "\n",
    "    # mean, std = calculate_dataset_satistics(target_loader)\n",
    "    # print(\"mean and std: \\n\", mean, std)\n",
    "\n",
    "    '''\n",
    "    image = Image.open(\"cityscapes/train/image/aachen/aachen_000000_000019_leftImg8bit.png\")\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "    pipeline = transforms.Compose([\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    x = pipeline(image)\n",
    "    '''\n",
    "\n",
    "    model.cuda()\n",
    "    # model.eval()\n",
    "    feature_source = 0\n",
    "    for idx, input in enumerate(tqdm(source_loader)):\n",
    "\n",
    "        features = model(input['image'].cuda())\n",
    "        feat = torch.flatten(features[0], start_dim=1)\n",
    "        if idx == 0:\n",
    "            feature_source = torch.zeros((len(source_loader), feat.shape[1]))\n",
    "            feature_source[idx, :] = feat\n",
    "            # feature_source = feat\n",
    "        else:\n",
    "            feature_source[idx, :] = feat\n",
    "            # feature_source = torch.cat((feature_source, feat), 0)\n",
    "\n",
    "    feature_target = 0\n",
    "    for idx, input in enumerate(tqdm(target_loader)):\n",
    "        features = model(input['image'].cuda())\n",
    "        feat = torch.flatten(features[0], start_dim=1)\n",
    "        if idx == 0:\n",
    "            feature_target = torch.zeros((len(target_loader), feat.shape[1]))\n",
    "            feature_target[idx, :] = feat\n",
    "            # feature_target = feat\n",
    "        else:\n",
    "            feature_target[idx, :] = feat\n",
    "            # feature_target = torch.cat((feature_target, feat), 0)\n",
    "    if mode == \"jigsaw\":\n",
    "        # torch.save(feature_target, 'target_jigsaw.pt')\n",
    "        feature_target = torch.load('target_jigsaw.pt')\n",
    "    else:\n",
    "        # torch.save(feature_target, 'target_rotnet.pt')\n",
    "        feature_target = torch.load('target_rotnet.pt')\n",
    "\n",
    "    fid = calculate_fid(feature_source, feature_target)\n",
    "    print(\"Epoch {}:\".format(opt.load_epoch), \"score\", fid, \"mode\", mode)\n",
    "\n",
    "    if mode == \"jigsaw\":\n",
    "        csv_path = os.path.join(os.getcwd(), \"results\", \"self_supervised_results_jigsaw_retouch_final.csv\")\n",
    "    else:\n",
    "        csv_path = os.path.join(os.getcwd(), \"results\", \"self_supervised_results_rotnet_retouch_final.csv\")\n",
    "\n",
    "    if os.path.isfile(csv_path):\n",
    "        x = []\n",
    "        value = []\n",
    "        with open(csv_path, 'r') as csvfile:\n",
    "            lines = csv.reader(csvfile, delimiter=',')\n",
    "            for idx, row in enumerate(lines):\n",
    "                if idx != 0:\n",
    "                    x.append(row[0])\n",
    "                    value.append(row[1])\n",
    "        x.append(opt.load_epoch)\n",
    "        value.append(fid)\n",
    "        x_np = np.asarray(x).astype(int)\n",
    "        value_np = np.asarray(value).astype(float)\n",
    "\n",
    "    to_write = []\n",
    "    if mode == \"jigsaw\":\n",
    "        to_write.append([\"epoch\", \"jigsaw\"])\n",
    "    else:\n",
    "        to_write.append([\"epoch\", \"rotnet\"])\n",
    "\n",
    "    if os.path.isfile(csv_path):\n",
    "        for epoch in range(len(x_np)):\n",
    "            result = [x_np[epoch], value_np[epoch]]\n",
    "            to_write.append(result)\n",
    "    else:\n",
    "        result = [opt.load_epoch, fid]\n",
    "        to_write.append(result)\n",
    "\n",
    "    with open(csv_path, \"w\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(to_write)\n",
    "\n",
    "\n",
    "def run_eval_ranked(epoch):\n",
    "    opt = argparse.ArgumentParser()  # get test options\n",
    "    # hard-code some parameters for test\n",
    "    opt.name = 'full_model'\n",
    "    opt.model = 'cycle_gan'\n",
    "\n",
    "    opt.num_threads = 0  # test code only supports num_threads = 0\n",
    "    opt.batch_size = 1  # test code only supports batch_size = 1\n",
    "    opt.serial_batches = True  # disable data shuffling; comment this line if results on randomly chosen images are needed.\n",
    "    opt.no_flip = True  # no flip; comment this line if results on flipped images are needed.\n",
    "    opt.display_id = -1  # no visdom display; the test code saves the results to a HTML file.\n",
    "\n",
    "    opt.netG = \"unet\"\n",
    "\n",
    "    opt.load_epoch = epoch\n",
    "\n",
    "    opt.base_dir = os.path.join(os.getcwd(), \"data/transferred/{}\".format(opt.load_epoch))\n",
    "    opt.list_dir = os.path.join(os.getcwd(), \"splits\")\n",
    "    source_dataset = Retouch_dataset(base_dir=opt.base_dir,\n",
    "                          list_dir=opt.list_dir,\n",
    "                          split='cirrus_samples',\n",
    "                          is_train=False)\n",
    "\n",
    "    source_loader = torch.utils.data.DataLoader(source_dataset,\n",
    "                                         1,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=opt.num_threads,\n",
    "                                         pin_memory=True,\n",
    "                                         drop_last=False)\n",
    "\n",
    "    opt.base_dir = os.path.join(os.getcwd(), \"data/retouch/\")\n",
    "    target_dataset = Retouch_dataset(base_dir=opt.base_dir,\n",
    "                          list_dir=opt.list_dir,\n",
    "                          split='spectralis_samples',\n",
    "                          is_train=False)\n",
    "\n",
    "    target_loader = torch.utils.data.DataLoader(target_dataset,\n",
    "                                         1,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=opt.num_threads,\n",
    "                                         pin_memory=True,\n",
    "                                         drop_last=False)\n",
    "\n",
    "    feature_target = 0\n",
    "    if epoch == 0:\n",
    "        for idx, input in enumerate(tqdm(target_loader)):\n",
    "\n",
    "            features = model(input['image'].cuda())\n",
    "            feat = torch.flatten(features[0], start_dim=1)\n",
    "            if idx == 0:\n",
    "                feature_target = torch.zeros((len(target_loader), feat.shape[1]))\n",
    "                feature_target[idx, :] = feat\n",
    "                # feature_target = feat\n",
    "            else:\n",
    "                feature_target[idx, :] = feat\n",
    "                # feature_target = torch.cat((feature_target, feat), 0)\n",
    "\n",
    "    if mode == \"jigsaw\":\n",
    "        if epoch == 0:\n",
    "            torch.save(feature_target, 'target_jigsaw.pt')\n",
    "        else:\n",
    "            feature_target = torch.load('target_jigsaw.pt')\n",
    "    else:\n",
    "        if epoch == 0:\n",
    "            torch.save(feature_target, 'target_rotnet.pt')\n",
    "        else:\n",
    "            feature_target = torch.load('target_rotnet.pt')\n",
    "\n",
    "    k = 100\n",
    "    feature_source = 0\n",
    "    for idx, input in enumerate(tqdm(source_loader)):\n",
    "        features = model(input['image'].cuda())\n",
    "        feat = torch.flatten(features[0], start_dim=1)\n",
    "\n",
    "        '''\n",
    "        dists = []\n",
    "        for row in feature_target:\n",
    "            feat_np = feat.detach().cpu().clone().numpy()\n",
    "            dists.append(spatial.distance.cosine(feat_np, row))\n",
    "        '''\n",
    "\n",
    "        feat_np = feat.detach().cpu().clone().numpy()\n",
    "        dists = cosine_similarity(feat_np, feature_target)\n",
    "\n",
    "        min_idx = np.argsort(dists)[:, -k:]\n",
    "        feat_target_sim = feature_target[min_idx, :].squeeze()\n",
    "\n",
    "        dist = calculate_diff(feat_np, feat_target_sim)\n",
    "        feature_source = feature_source + dist\n",
    "\n",
    "    fid = feature_source.item() / len(source_loader)\n",
    "    print(\"FID score\", fid)\n",
    "\n",
    "    if mode == \"jigsaw\":\n",
    "        csv_path = os.path.join(os.getcwd(), \"results/OCT_full\", \"self_supervised_results_jigsaw_OCT_full_ranked.csv\")\n",
    "    else:\n",
    "        csv_path = os.path.join(os.getcwd(), \"results/OCT_full\", \"self_supervised_results_rotnet_OCT_full_ranked.csv\")\n",
    "\n",
    "    if os.path.isfile(csv_path):\n",
    "        x = []\n",
    "        value = []\n",
    "        with open(csv_path, 'r') as csvfile:\n",
    "            lines = csv.reader(csvfile, delimiter=',')\n",
    "            for idx, row in enumerate(lines):\n",
    "                if idx != 0:\n",
    "                    x.append(row[0])\n",
    "                    value.append(row[1])\n",
    "        x.append(opt.load_epoch)\n",
    "        value.append(fid)\n",
    "        x_np = np.asarray(x).astype(int)\n",
    "        value_np = np.asarray(value).astype(float)\n",
    "\n",
    "    to_write = []\n",
    "    if mode == \"jigsaw\":\n",
    "        to_write.append([\"epoch\", \"jigsaw\"])\n",
    "    else:\n",
    "        to_write.append([\"epoch\", \"rotnet\"])\n",
    "\n",
    "    if os.path.isfile(csv_path):\n",
    "        for epoch in range(len(x_np)):\n",
    "            result = [x_np[epoch], value_np[epoch]]\n",
    "            to_write.append(result)\n",
    "    else:\n",
    "        result = [opt.load_epoch, fid]\n",
    "        to_write.append(result)\n",
    "\n",
    "    with open(csv_path, \"w\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the correct config file:\n",
    "```\n",
    "jigsaw_custom_retouch\n",
    "jigsaw_custom_cityscapes\n",
    "jigsaw_custom_mnist\n",
    "jigsaw_custom_synthia\n",
    "rotnet_custom_retouch\n",
    "rotnet_custom_cityscapes\n",
    "rotnet_custom_mnist\n",
    "rotnet_custom_synthia\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Checkpoint path checkpoints/model_phase100.torch not found\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m weights \u001b[38;5;241m=\u001b[39m load_checkpoint(checkpoint_path\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mWEIGHTS_INIT\u001b[38;5;241m.\u001b[39mPARAMS_FILE)\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 21\u001b[0m \u001b[43minit_model_from_consolidated_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_dict_key_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclassy_state_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use this if you do not want to load all layers\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m transferred_images_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/transferred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m epochs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(transferred_images_dir) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(transferred_images_dir, f))]\n",
      "File \u001b[0;32m~/miniconda3/envs/testing/lib/python3.8/site-packages/vissl/utils/checkpoint.py:827\u001b[0m, in \u001b[0;36minit_model_from_consolidated_weights\u001b[0;34m(config, model, state_dict, state_dict_key_name, skip_layers, replace_prefix, append_prefix)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# whether it's a model from somewhere else or a model from this codebase, load the\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;66;03m# state_dict\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_dict_key_name \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state_dict_key_name) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m--> 827\u001b[0m         state_dict_key_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m()\n\u001b[1;32m    828\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown state dict key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate_dict_key_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    829\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m state_dict[state_dict_key_name]\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_dict_key_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassy_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;66;03m# get the appropriate model_state_dict so that the model can load. We automatically\u001b[39;00m\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;66;03m# take care of appending prefixes, suffixes etc to match the layer names.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "mode = \"jigsaw\" # rotnet / simclr\n",
    "\n",
    "config = OmegaConf.load(\"configs/config/jigsaw_custom_retouch.yaml\")\n",
    "\n",
    "default_config = OmegaConf.load(\"configs/config/defaults.yaml\")\n",
    "cfg = OmegaConf.merge(default_config, config)\n",
    "\n",
    "cfg = AttrDict(cfg)\n",
    "cfg.config.MODEL._MODEL_INIT_SEED = 0\n",
    "cfg.config.MODEL.WEIGHTS_INIT.PARAMS_FILE = \"checkpoints/model_phase100.torch\"\n",
    "cfg.config.MODEL.FEATURE_EVAL_SETTINGS.EVAL_MODE_ON = True\n",
    "cfg.config.MODEL.FEATURE_EVAL_SETTINGS.FREEZE_TRUNK_ONLY = True\n",
    "cfg.config.MODEL.FEATURE_EVAL_SETTINGS.EXTRACT_TRUNK_FEATURES_ONLY = True\n",
    "cfg.config.MODEL.FEATURE_EVAL_SETTINGS.SHOULD_FLATTEN_FEATS = False\n",
    "cfg.config.MODEL.FEATURE_EVAL_SETTINGS.LINEAR_EVAL_FEAT_POOL_OPS_MAP = [[\"res5avg\", [\"Identity\", []]]]\n",
    "\n",
    "model = build_model(cfg.config.MODEL, cfg.config.OPTIMIZER)\n",
    "weights = load_checkpoint(checkpoint_path=cfg.config.MODEL.WEIGHTS_INIT.PARAMS_FILE)\n",
    "model.cuda()\n",
    "\n",
    "init_model_from_consolidated_weights(\n",
    "    config=cfg.config,\n",
    "    model=model,\n",
    "    state_dict=weights,\n",
    "    state_dict_key_name=\"classy_state_dict\",\n",
    "    skip_layers=[],  # Use this if you do not want to load all layers\n",
    ")\n",
    "\n",
    "transferred_images_dir = os.path.join(os.getcwd(), \"data/transferred\")\n",
    "epochs = [int(f) for f in os.listdir(transferred_images_dir) if os.path.isdir(os.path.join(transferred_images_dir, f))]\n",
    "epochs.sort()\n",
    "\n",
    "for epoch in epochs:\n",
    "    print(\"run eval epoch {}\".format(epoch))\n",
    "    run_eval(int(epoch))\n",
    "    # run_eval_ranked(int(folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Feature Extraction V0.1.6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
