{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/facebookresearch/vissl/blob/v0.1.6/tutorials/Feature_Extraction_V0_1_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1ndZ6XwI7MYA"
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XzxTZfKwFNo"
   },
   "source": [
    "# Feature Extraction\n",
    "\n",
    "In this tutorial, we look at a simple example of how to use VISSL to extract features after finished training the vissl moddels.\n",
    "\n",
    "**EXAMPLE 1**: Download the pre-trained [jigsaw-retouch](https://drive.google.com/file/d/159SgjqklmLHWpEQNq14i_gJk0NDhyAHE/view?usp=sharing) to the `root` directory and rename it to `checkpoints`.\n",
    "\n",
    "**EXAMPLE 2**: Download the pre-trained [jigsaw-cityscapes](https://drive.google.com/file/d/1Af710oLe_n1h4RMMnhdbxWQWiDCJx68j/view?usp=sharing) to the `root` directory and rename it to `checkpoints`.\n",
    "\n",
    "VISSL should be successfuly installed by now and all the dependencies should be available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Np6atgoOTPrA"
   },
   "outputs": [],
   "source": [
    "import vissl\n",
    "import tensorboard\n",
    "import apex\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the custom data in VISSL\n",
    "\n",
    "The original data is saved in the `data` directory. The transferred images are saved in such a way, that they are stored in the `data/generated_images/#epoch` directory (`#epoch` is the number of CycleGAN epoch).\n",
    "\n",
    "**EXAMPLE 1**: download the retouch data set from [retouch-dataset](https://drive.google.com/file/d/1r8pQCoVzEAHdy9wLW_MUkyfgBBFePMPv/view?usp=sharing) and insert it into the `data/real_images` directory. Download the transferred images from [transferred-retouch-images](https://drive.google.com/file/d/1nMcyF-z2yvPBDY70qBsT2Ydg7NUITpmR/view?usp=sharing) and isert the subfolders with the epoch number into the `data/generated_images` directory.\n",
    "\n",
    "**EXAMPLE 2**: download the truncated retouch GTAV data set from [gta5-truncated-dataset](https://drive.google.com/file/d/1R9zmrwAKf03KOq9MSfhdPd6xOVRGEtrY/view?usp=sharing) and insert it into the `data/real_images` directory. Download the transferred images from [transferred-gta5-images](https://drive.google.com/file/d/1SLdGNHDi3LZTHXXNMNFDTmAQibAEjj-x/view?usp=sharing) and isert the subfolders with the epoch number into the `data/generated_images` directory. Note, it also works with the whole data set, one only has to change the `splits/gta5.txt` to the whole dataset. The truncated version is used due to memory and time efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_526152/154125973.py:6: DeprecationWarning: Please use `zoom` from the `scipy.ndimage` namespace, the `scipy.ndimage.interpolation` namespace is deprecated.\n",
      "  from scipy.ndimage.interpolation import zoom\n"
     ]
    }
   ],
   "source": [
    "import skimage.io as io\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from torch.utils.data import Dataset\n",
    "# from medpy.io import load\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image  # using pillow-simd for increased speed\n",
    "\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning\n",
    "    # (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert(\"L\")\n",
    "\n",
    "class Retouch_dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 base_dir,\n",
    "                 list_dir,\n",
    "                 size=(512, 512),\n",
    "                 is_train=False,\n",
    "                 transform=None,\n",
    "                 ext='.png'):\n",
    "        self.transform = transform  # using transform in torch!\n",
    "        self.sample_list = open(list_dir).readlines()\n",
    "\n",
    "        self.data_dir = base_dir\n",
    "        self.loader = pil_loader\n",
    "        self.to_tensor = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size),\n",
    "                transforms.ToTensor(),\n",
    "                #transforms.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "            ])\n",
    "\n",
    "        self.is_train = is_train\n",
    "        self.transform = transform\n",
    "        self.ext = ext\n",
    "\n",
    "    def augment(self, data, label):\n",
    "        data_label = torch.cat((data, label), dim=0)\n",
    "        data_label_aug = self.transform(data_label)\n",
    "        data_aug = data_label_aug[0, :, :].unsqueeze(0)\n",
    "        label_aug = data_label_aug[1, :, :].unsqueeze(0)\n",
    "        return data_aug, label_aug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_name = self.sample_list[idx].strip('\\n')\n",
    "\n",
    "        vendor = sample_name.split(' ')[0]\n",
    "        slice_name = sample_name.split(' ')[1]\n",
    "        slice_idx = sample_name.split(' ')[2].zfill(3)\n",
    "\n",
    "        data_path = os.path.join(self.data_dir,\n",
    "                                 vendor,\n",
    "                                 slice_name,\n",
    "                                 'images',\n",
    "                                 slice_idx + self.ext)\n",
    "        '''\n",
    "\n",
    "        label_path = os.path.join(self.data_dir,\n",
    "                                  vendor,\n",
    "                                  slice_name,\n",
    "                                  'labels',\n",
    "                                  slice_idx + '.npy')\n",
    "\n",
    "        label = torch.from_numpy(np.load(label_path))\n",
    "        label_idx = torch.argmax(label, dim=0, keepdim=True)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        data = self.to_tensor(self.loader(data_path))\n",
    "        transform_avaliable = self.transform is not None and self.is_train\n",
    "        do_aug = transform_avaliable and random.random() > 0.5\n",
    "\n",
    "        if do_aug:\n",
    "            # data, label_idx = self.augment(data, label_idx)\n",
    "            data = self.augment(data)\n",
    "\n",
    "        # label_idx = label_idx.squeeze(0).long()\n",
    "\n",
    "        sample = {'image': data.repeat(3, 1, 1),\n",
    "                  'case_name': sample_name}\n",
    "        # print((label_idx==0).sum()/512**2)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gta5 dataset (source)\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "'''\n",
    "labels = [\n",
    "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
    "    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
    "    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
    "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
    "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
    "    Label(  'parking'              ,  9 ,      255 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
    "    Label(  'rail track'           , 10 ,      255 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
    "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
    "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
    "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
    "    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
    "    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
    "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
    "    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
    "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
    "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
    "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
    "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
    "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
    "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
    "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
    "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
    "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
    "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
    "    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
    "    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
    "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
    "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
    "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
    "    Label(  'license plate'        , -1 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
    "]\n",
    "'''\n",
    "\n",
    "class GTA5Dataset(data.Dataset):\n",
    "    def __init__(self, root, list_path, max_iters=None, crop_size=(256, 256), mean=(128, 128, 128), ignore_label=255):\n",
    "        self.root = root\n",
    "        self.list_path = list_path\n",
    "        self.crop_size = crop_size\n",
    "        self.ignore_label = ignore_label\n",
    "        self.mean = mean\n",
    "        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n",
    "        if not max_iters==None:\n",
    "            self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))\n",
    "        self.files = []\n",
    "\n",
    "        self.id_to_trainid = {7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,\n",
    "                              19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12,\n",
    "                              26: 13, 27: 14, 28: 15, 31: 16, 32: 17, 33: 18}\n",
    "\n",
    "        self.id2label = {-1: ignore_label, 0: ignore_label, 1: ignore_label, 2: ignore_label,\n",
    "            3: ignore_label, 4: ignore_label, 5: ignore_label, 6: ignore_label,\n",
    "            7: 0, 8: 1, 9: ignore_label, 10: ignore_label, 11: 2, 12: 3, 13: 4,\n",
    "            14: ignore_label, 15: ignore_label, 16: ignore_label, 17: 5,\n",
    "            18: ignore_label, 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14,\n",
    "            28: 15, 29: ignore_label, 30: ignore_label, 31: 16, 32: 17, 33: 18}\n",
    "\n",
    "        #self.id_to_trainid = {7: 1, 24: 2, 26: 3} #Road/car/people\n",
    "        self.id_to_trainid = {11: 1, 24: 2, 21: 3} #Building/car/vegetation\n",
    "        #self.ignore_label = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name = self.img_ids[index]\n",
    "        \n",
    "        # name = name[:-4] + \".jpg\"\n",
    "        \n",
    "        image = Image.open(osp.join(self.root, \"images/%s\" % name)).convert('RGB')\n",
    "        label = Image.open(osp.join(self.root, \"labels/%s\" % name))\n",
    "        # resize\n",
    "        image = image.resize(self.crop_size, Image.BICUBIC)\n",
    "        label = label.resize(self.crop_size, Image.NEAREST)\n",
    "\n",
    "        image = np.asarray(image, np.float32)\n",
    "        label = np.asarray(label, np.int8)\n",
    "\n",
    "        label_copy = self.ignore_label * np.ones(label.shape, dtype=np.float32)\n",
    "        #for k, v in self.id_to_trainid.items():\n",
    "        for k, v in self.id2label.items():\n",
    "            label_copy[label == k] = v\n",
    "        size = image.shape\n",
    "        #image = image[:, :, ::-1]  # change to BGR\n",
    "        #image -= self.mean\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        sample = {'image': image.copy(),\n",
    "                  'label': label_copy.copy()}\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cityscapes dataset (target)\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "palette = [128, 64, 128, 244, 35, 232, 70, 70, 70, 102, 102, 156, 190, 153, 153, 153, 153, 153, 250, 170, 30,\n",
    "            220, 220, 0, 107, 142, 35, 152, 251, 152, 70, 130, 180, 220, 20, 60, 255, 0, 0, 0, 0, 142, 0, 0, 70,\n",
    "            0, 60, 100, 0, 80, 100, 0, 0, 230, 119, 11, 32]\n",
    "classes = ['road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light', 'traffic sign',\n",
    "        'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck', 'bus', 'train', 'motorcycle',\n",
    "        'bicycle']\n",
    "\n",
    "    \n",
    "class CityscapesDataset(data.Dataset):\n",
    "    def __init__(self, root, list_path, max_iters=None, crop_size=(256, 256), mean=(128, 128, 128), ignore_label=255):\n",
    "        self.root = root\n",
    "        self.list_path = list_path\n",
    "        self.crop_size = crop_size\n",
    "        self.ignore_label = ignore_label\n",
    "        self.mean = mean\n",
    "        self.img_ids = [i_id.strip() for i_id in open(list_path)]\n",
    "        if not max_iters==None:\n",
    "            self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))\n",
    "        self.files = []\n",
    "        \n",
    "        self.id_to_trainid = {7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,\n",
    "                              19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12,\n",
    "                              26: 13, 27: 14, 28: 15, 31: 16, 32: 17, 33: 18}\n",
    "\n",
    "        self.id2label = {-1: ignore_label, 0: ignore_label, 1: ignore_label, 2: ignore_label,\n",
    "            3: ignore_label, 4: ignore_label, 5: ignore_label, 6: ignore_label,\n",
    "            7: 0, 8: 1, 9: ignore_label, 10: ignore_label, 11: 2, 12: 3, 13: 4,\n",
    "            14: ignore_label, 15: ignore_label, 16: ignore_label, 17: 5,\n",
    "            18: ignore_label, 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14,\n",
    "            28: 15, 29: ignore_label, 30: ignore_label, 31: 16, 32: 17, 33: 18}\n",
    "\n",
    "        #self.id_to_trainid = {7: 1, 24: 2, 26: 3} #Road/car/people\n",
    "        self.id_to_trainid = {11: 1, 24: 2, 21: 3} #Building/car/vegetation\n",
    "        #self.ignore_label = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name = self.img_ids[index]\n",
    "        image_root = osp.join(self.root, 'cityscapes')\n",
    "        label_root = osp.join(self.root, 'gtFine')\n",
    "        image = Image.open(osp.join(image_root, \"%s\" % name)).convert('RGB')\n",
    "        label = Image.open(osp.join(label_root, \"%s\" % name.replace(\"leftImg8bit\", \"gtFine_labelIds\")))\n",
    "        # resize\n",
    "        image = image.resize(self.crop_size, Image.BICUBIC)\n",
    "        label = label.resize(self.crop_size, Image.NEAREST)\n",
    "\n",
    "        image = np.asarray(image, np.float32)\n",
    "        label = np.asarray(label, np.int8)\n",
    "\n",
    "        label_copy = self.ignore_label * np.ones(label.shape, dtype=np.float32)\n",
    "        #for k, v in self.id_to_trainid.items():\n",
    "        for k, v in self.id2label.items():\n",
    "            label_copy[label == k] = v\n",
    "        size = image.shape\n",
    "        #image = image[:, :, ::-1]  # change to BGR\n",
    "        #image -= self.mean\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        sample = {'image': image.copy(),\n",
    "                  'label': label_copy.copy()}\n",
    "\n",
    "        return sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from vissl.utils.hydra_config import AttrDict\n",
    "from vissl.models import build_model\n",
    "from classy_vision.generic.util import load_checkpoint\n",
    "from vissl.utils.checkpoint import init_model_from_consolidated_weights\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import argparse\n",
    "\n",
    "from datasets import create_dataset\n",
    "\n",
    "import sys, os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import linalg\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def calculate_fid(feat1, feat2):\n",
    "    \"\"\" Calculate FID between images1 and images2\n",
    "    Args:\n",
    "        images1: np.array, shape: (N, H, W, 3), dtype: np.float32 between 0-1 or np.uint8\n",
    "        images2: np.array, shape: (N, H, W, 3), dtype: np.float32 between 0-1 or np.uint8\n",
    "        use_multiprocessing: If multiprocessing should be used to pre-process the images\n",
    "        batch size: batch size used for inception network\n",
    "    Returns:\n",
    "        FID (scalar)\n",
    "    \"\"\"\n",
    "    mu1, sigma1 = calculate_activation_statistics(feat1)\n",
    "    mu2, sigma2 = calculate_activation_statistics(feat2)\n",
    "    fid = calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\n",
    "\n",
    "    return fid\n",
    "\n",
    "\n",
    "def calculate_activation_statistics(feat):\n",
    "    \"\"\"Calculates the statistics used by FID\n",
    "    Args:\n",
    "        feat: torch.tensor, shape: (N, 2048), dtype: torch.float32 in range 0 - 1\n",
    "    Returns:\n",
    "        mu:     mean over all activations from the last pool layer of the inception model\n",
    "        sigma:  covariance matrix over all activations from the last pool layer\n",
    "                of the inception model.\n",
    "    \"\"\"\n",
    "\n",
    "    feat_np = feat.cpu().detach().numpy()\n",
    "    mu = np.mean(feat_np, axis=0) # (2048, 0)\n",
    "    sigma = np.cov(feat_np, rowvar=False) # (2048, 2048)\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "# Modified from: https://github.com/bioinf-jku/TTUR/blob/master/fid.py\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\n",
    "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
    "    and X_2 ~ N(mu_2, C_2) is\n",
    "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "\n",
    "    Stable version by Dougal J. Sutherland.\n",
    "    Params:\n",
    "    -- mu1 : Numpy array containing the activations of the pool_3 layer of the\n",
    "             inception net ( like returned by the function 'get_predictions')\n",
    "             for generated samples.\n",
    "    -- mu2   : The sample mean over activations of the pool_3 layer, precalcualted\n",
    "               on an representive data set.\n",
    "    -- sigma1: The covariance matrix over activations of the pool_3 layer for\n",
    "               generated samples.\n",
    "    -- sigma2: The covariance matrix over activations of the pool_3 layer,\n",
    "               precalcualted on an representive data set.\n",
    "    Returns:\n",
    "    --   : The Frechet Distance.\n",
    "    \"\"\"\n",
    "\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\n",
    "    assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "    # product might be almost singular\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = \"fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps\n",
    "        print(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    # numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError(\"Imaginary component {}\".format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
    "\n",
    "\n",
    "def create_dataset(dataset_mode, folder_name, sample_list, size):\n",
    "    if dataset_mode == \"OCT\":\n",
    "        dataset = Retouch_dataset(base_dir=folder_name, list_dir=sample_list, size = size)\n",
    "    elif dataset_mode == \"gta5\":\n",
    "        dataset = GTA5Dataset(root=folder_name, list_path=sample_list, crop_size=size, ignore_label=19)\n",
    "    elif dataset_mode == \"cityscapes\":\n",
    "        dataset = CityscapesDataset(root=folder_name, list_path=sample_list, crop_size=size, ignore_label=19)\n",
    "    else:\n",
    "        print(\"Unrecognized dataset!\")\n",
    "        sys.exit()\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def run_eval(opt):\n",
    "    real_dataset = create_dataset(opt.dataset_mode, opt.real_dir, opt.real_list, opt.crop_size)\n",
    "    fake_dataset = create_dataset(opt.dataset_mode, opt.fake_dir, opt.fake_list, opt.crop_size)\n",
    "\n",
    "    real_loader = torch.utils.data.DataLoader(real_dataset,\n",
    "                                         batch_size=opt.batch_size,\n",
    "                                         shuffle=opt.shuffle,\n",
    "                                         num_workers=opt.num_threads,\n",
    "                                         pin_memory=True,\n",
    "                                         drop_last=False)\n",
    "\n",
    "    fake_loader = torch.utils.data.DataLoader(fake_dataset,\n",
    "                                         batch_size=opt.batch_size,\n",
    "                                         shuffle=opt.shuffle,\n",
    "                                         num_workers=opt.num_threads,\n",
    "                                         pin_memory=True,\n",
    "                                         drop_last=False)\n",
    "    \n",
    "    model.cuda()\n",
    "    # model.eval()\n",
    "    feature_fake = 0\n",
    "    for idx, input in enumerate(tqdm(fake_loader)):\n",
    "        features = model(input['image'].cuda())\n",
    "        feat = torch.flatten(features[0], start_dim=1)\n",
    "        if idx == 0:\n",
    "            feature_fake = torch.zeros((len(fake_loader), feat.shape[1]))\n",
    "            feature_fake[idx, :] = feat\n",
    "        else:\n",
    "            feature_fake[idx, :] = feat\n",
    "\n",
    "    feature_real = 0\n",
    "    if not os.path.exists(\"real.pt\"):\n",
    "        for idx, input in enumerate(tqdm(real_loader)):\n",
    "            features = model(input['image'].cuda())\n",
    "            feat = torch.flatten(features[0], start_dim=1)\n",
    "            if idx == 0:\n",
    "                feature_real = torch.zeros((len(real_loader), feat.shape[1]))\n",
    "                feature_real[idx, :] = feat\n",
    "                # feature_target = feat\n",
    "            else:\n",
    "                feature_real[idx, :] = feat\n",
    "                # feature_target = torch.cat((feature_target, feat), 0)\n",
    "            \n",
    "        torch.save(feature_real, 'real.pt')\n",
    "    else:\n",
    "        feature_real = torch.load('real.pt')\n",
    "\n",
    "    fid = calculate_fid(feature_fake, feature_real)\n",
    "    print(\"Epoch {}:\".format(opt.load_epoch), \"score\", fid)\n",
    "\n",
    "    csv_path = os.path.join(os.getcwd(), \"results\", \"self_supervised_results_{}.csv\".format(opt.dataset_mode))\n",
    "   \n",
    "    if os.path.isfile(csv_path):\n",
    "        x = []\n",
    "        value = []\n",
    "        with open(csv_path, 'r') as csvfile:\n",
    "            lines = csv.reader(csvfile, delimiter=',')\n",
    "            for idx, row in enumerate(lines):\n",
    "                if idx != 0:\n",
    "                    x.append(row[0])\n",
    "                    value.append(row[1])\n",
    "        x.append(opt.load_epoch)\n",
    "        value.append(fid)\n",
    "        x_np = np.asarray(x).astype(int)\n",
    "        value_np = np.asarray(value).astype(float)\n",
    "\n",
    "    to_write = []\n",
    "    to_write.append([\"epoch\", \"jigsaw\"])\n",
    "\n",
    "    if os.path.isfile(csv_path):\n",
    "        for epoch in range(len(x_np)):\n",
    "            result = [x_np[epoch], value_np[epoch]]\n",
    "            to_write.append(result)\n",
    "    else:\n",
    "        result = [opt.load_epoch, fid]\n",
    "        to_write.append(result)\n",
    "\n",
    "    with open(csv_path, \"w\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(to_write)\n",
    "        x = []\n",
    "        value = []\n",
    "        with open(csv_path, 'r') as csvfile:\n",
    "            lines = csv.reader(csvfile, delimiter=',')\n",
    "            for idx, row in enumerate(lines):\n",
    "                if idx != 0:\n",
    "                    x.append(row[0])\n",
    "                    value.append(row[1])\n",
    "        x.append(opt.load_epoch)\n",
    "        value.append(fid)\n",
    "        x_np = np.asarray(x).astype(int)\n",
    "        value_np = np.asarray(value).astype(float)\n",
    "\n",
    "    to_write = []\n",
    "    to_write.append([\"epoch\", \"jigsaw\"])\n",
    "\n",
    "    if os.path.isfile(csv_path):\n",
    "        for epoch in range(len(x_np)):\n",
    "            result = [x_np[epoch], value_np[epoch]]\n",
    "            to_write.append(result)\n",
    "    else:\n",
    "        result = [opt.load_epoch, fid]\n",
    "        to_write.append(result)\n",
    "\n",
    "    with open(csv_path, \"w\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(to_write)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the correct config file:\n",
    "```\n",
    "jigsaw_custom_retouch\n",
    "jigsaw_custom_cityscapes\n",
    "jigsaw_custom_mnist\n",
    "jigsaw_custom_synthia\n",
    "rotnet_custom_retouch\n",
    "rotnet_custom_cityscapes\n",
    "rotnet_custom_mnist\n",
    "rotnet_custom_synthia\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run eval epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3072/3072 [01:06<00:00, 45.91it/s]\n",
      "100%|███████████████████████████████████████| 1176/1176 [00:24<00:00, 47.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: score 2.420683817463056\n",
      "run eval epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/3072 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/zeju/Documents/Zeiss_Self_Supervised/data/generated_images/OCT_new/0/3/Cirrus_part3/d40ef678ef3dff06eb3aa3a4f4ae99e6/images/006.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m opt\u001b[38;5;241m.\u001b[39mfake_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(opt\u001b[38;5;241m.\u001b[39mfake_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch))\n\u001b[1;32m     60\u001b[0m opt\u001b[38;5;241m.\u001b[39mload_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(epoch)\n\u001b[0;32m---> 61\u001b[0m \u001b[43mrun_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mrun_eval\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# model.eval()\u001b[39;00m\n\u001b[1;32m    146\u001b[0m feature_fake \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(fake_loader)):\n\u001b[1;32m    148\u001b[0m     features \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;28minput\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda())\n\u001b[1;32m    149\u001b[0m     feat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(features[\u001b[38;5;241m0\u001b[39m], start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/testing/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/testing/lib/python3.8/site-packages/torch/utils/data/dataloader.py:517\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 517\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/testing/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    556\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    559\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/testing/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 44\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/testing/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 44\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mRetouch_dataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     67\u001b[0m data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir,\n\u001b[1;32m     68\u001b[0m                          vendor,\n\u001b[1;32m     69\u001b[0m                          slice_name,\n\u001b[1;32m     70\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     71\u001b[0m                          slice_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mext)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03mlabel_path = os.path.join(self.data_dir,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     86\u001b[0m transform_avaliable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_train\n\u001b[1;32m     87\u001b[0m do_aug \u001b[38;5;241m=\u001b[39m transform_avaliable \u001b[38;5;129;01mand\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_loader\u001b[39m(path):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mopen(f) \u001b[38;5;28;01mas\u001b[39;00m img:\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/zeju/Documents/Zeiss_Self_Supervised/data/generated_images/OCT_new/0/3/Cirrus_part3/d40ef678ef3dff06eb3aa3a4f4ae99e6/images/006.png'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    config = OmegaConf.load(\"configs/config/jigsaw_custom_retouch.yaml\")\n",
    "\n",
    "    default_config = OmegaConf.load(\"configs/config/defaults.yaml\")\n",
    "    cfg = OmegaConf.merge(default_config, config)\n",
    "\n",
    "    cfg = AttrDict(cfg)\n",
    "    cfg.config.MODEL._MODEL_INIT_SEED = 0\n",
    "    cfg.config.MODEL.WEIGHTS_INIT.PARAMS_FILE = \"./checkpoints/model_phase100.torch\"\n",
    "    cfg.config.MODEL.FEATURE_EVAL_SETTINGS.EVAL_MODE_ON = True\n",
    "    cfg.config.MODEL.FEATURE_EVAL_SETTINGS.FREEZE_TRUNK_ONLY = True\n",
    "    cfg.config.MODEL.FEATURE_EVAL_SETTINGS.EXTRACT_TRUNK_FEATURES_ONLY = True\n",
    "    cfg.config.MODEL.FEATURE_EVAL_SETTINGS.SHOULD_FLATTEN_FEATS = False\n",
    "    cfg.config.MODEL.FEATURE_EVAL_SETTINGS.LINEAR_EVAL_FEAT_POOL_OPS_MAP = [[\"res5avg\", [\"Identity\", []]]]\n",
    "\n",
    "    model = build_model(cfg.config.MODEL, cfg.config.OPTIMIZER)\n",
    "    weights = load_checkpoint(checkpoint_path=cfg.config.MODEL.WEIGHTS_INIT.PARAMS_FILE)\n",
    "    model.cuda()\n",
    "\n",
    "    init_model_from_consolidated_weights(\n",
    "        config=cfg.config,\n",
    "        model=model,\n",
    "        state_dict=weights,\n",
    "        state_dict_key_name=\"classy_state_dict\",\n",
    "        skip_layers=[],  # Use this if you do not want to load all layers\n",
    "    )\n",
    "    \n",
    "    opt = argparse.ArgumentParser()\n",
    "    opt.dataset_mode = \"OCT\"\n",
    "    opt.real_dir = os.path.join(os.getcwd(), \"data/real_images/retouch-dataset\")\n",
    "    opt.fake_dir = os.path.join(os.getcwd(), \"data/generated_images/OCT_new\")\n",
    "    opt.load_epoch = 0\n",
    "    \n",
    "    opt.real_list = os.path.join(os.getcwd(), \"splits/spectralis_samples.txt\")\n",
    "    opt.fake_list = os.path.join(os.getcwd(), \"splits/cirrus_samples.txt\")\n",
    "    \n",
    "    opt.crop_size= (512, 512)\n",
    "    \n",
    "    opt.num_threads = 0  \n",
    "    opt.batch_size = 1 \n",
    "    opt.shuffle = True  \n",
    "    opt.no_flip = True  \n",
    "    opt.display_id = -1  \n",
    "\n",
    "    head = \"results/\"\n",
    "\n",
    "    if not os.path.exists(head):\n",
    "        os.makedirs(head)\n",
    "\n",
    "    transferred_images_dir = opt.fake_dir\n",
    "    epochs = [int(f) for f in os.listdir(transferred_images_dir) if os.path.isdir(os.path.join(transferred_images_dir, f))]\n",
    "    epochs.sort()\n",
    "\n",
    "    if os.path.exists(\"real.pt\"):\n",
    "        os.remove(\"real.pt\")\n",
    "\n",
    "    for epoch in epochs:\n",
    "        print(\"run eval epoch {}\".format(epoch))\n",
    "        opt.fake_dir = os.path.join(opt.fake_dir, \"{}\".format(epoch))\n",
    "        opt.load_epoch = int(epoch)\n",
    "        run_eval(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Feature Extraction V0.1.6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
